{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Transformer on tf-idf Features on Additional and Main Dataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqC104C0UDKI",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:20:56.713276Z",
          "iopub.execute_input": "2021-09-18T09:20:56.713672Z",
          "iopub.status.idle": "2021-09-18T09:20:57.392359Z",
          "shell.execute_reply.started": "2021-09-18T09:20:56.713593Z",
          "shell.execute_reply": "2021-09-18T09:20:57.391377Z"
        },
        "trusted": true,
        "outputId": "bfcc7182-77f3-4f2b-d06c-e35588343dbb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Sat Sep 18 09:20:57 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2HsFgn9UTmf",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:00.413037Z",
          "iopub.execute_input": "2021-09-18T09:21:00.413372Z",
          "iopub.status.idle": "2021-09-18T09:21:26.852716Z",
          "shell.execute_reply.started": "2021-09-18T09:21:00.413336Z",
          "shell.execute_reply": "2021-09-18T09:21:26.851819Z"
        },
        "trusted": true,
        "outputId": "e915c0a5-f330-4316-f540-014197d9a547"
      },
      "source": [
        "!pip install pandarallel -q\n",
        "!pip install neptune-client -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.21.44 which is incompatible.\u001b[0m\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOY5ii5TUXWy"
      },
      "source": [
        "# Importing the Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10O06vRjUTkK",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:26.854573Z",
          "iopub.execute_input": "2021-09-18T09:21:26.854927Z",
          "iopub.status.idle": "2021-09-18T09:21:34.512440Z",
          "shell.execute_reply.started": "2021-09-18T09:21:26.854887Z",
          "shell.execute_reply": "2021-09-18T09:21:34.511075Z"
        },
        "trusted": true,
        "outputId": "06f4cb67-50e7-49ab-d39a-8cb77947814d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import random as rn\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,average_precision_score, precision_score,precision_recall_curve\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm import trange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "import nltk\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import neptune.new as neptune\n",
        "\n",
        "from torch.utils.data import (DataLoader, RandomSampler, WeightedRandomSampler, SequentialSampler, TensorDataset)\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "# Initialization\n",
        "pandarallel.initialize(progress_bar = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "INFO: Pandarallel will run on 2 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUnEDSX6UTh1",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:34.514316Z",
          "iopub.execute_input": "2021-09-18T09:21:34.514684Z",
          "iopub.status.idle": "2021-09-18T09:21:35.694758Z",
          "shell.execute_reply.started": "2021-09-18T09:21:34.514647Z",
          "shell.execute_reply": "2021-09-18T09:21:35.693993Z"
        },
        "trusted": true,
        "outputId": "2e8fc176-7444-48f0-8a4d-c083265da21b"
      },
      "source": [
        "run = neptune.init(project='manav0211/emnlp',\n",
        "                   tags = 'alberta large seed 40 on dependency features',\n",
        "                   api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkMzUxMGNhMS00N2E5LTQ2YmUtYWI1Yi03ZDFhZTEzODg1NmEifQ==') # your credentials"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "https://app.neptune.ai/manav0211/emnlp/e/EM-476\nRemember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qzlV5IpUTfy",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:35.696820Z",
          "iopub.execute_input": "2021-09-18T09:21:35.697434Z",
          "iopub.status.idle": "2021-09-18T09:21:35.706417Z",
          "shell.execute_reply.started": "2021-09-18T09:21:35.697391Z",
          "shell.execute_reply": "2021-09-18T09:21:35.705486Z"
        },
        "trusted": true
      },
      "source": [
        "SEED = 1\n",
        "rn.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_-fQB9DUTdC",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:35.708481Z",
          "iopub.execute_input": "2021-09-18T09:21:35.710579Z",
          "iopub.status.idle": "2021-09-18T09:21:35.716063Z",
          "shell.execute_reply.started": "2021-09-18T09:21:35.710532Z",
          "shell.execute_reply": "2021-09-18T09:21:35.715055Z"
        },
        "trusted": true
      },
      "source": [
        "path_dataset = '../input/emnlp-full-dataset/'\n",
        "\n",
        "# use the below two lines for storing model on noun features and comment the next two lined\n",
        "#path_predictions_folder = '/content/drive/MyDrive/EMNLP_folder_4/noun_features_model_predictions_combined_dataset/'\n",
        "#save_model_folder = '/content/drive/MyDrive/EMNLP_folder_4/noun_features_model_files_combined_dataset/'\n",
        "\n",
        "# use the below two lines for storing model on dependency features and comment the previous two lines\n",
        "\n",
        "path_predictions_folder = ''\n",
        "save_model_folder = '' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqlgnES8UTam",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:35.717470Z",
          "iopub.execute_input": "2021-09-18T09:21:35.717932Z",
          "iopub.status.idle": "2021-09-18T09:21:35.732828Z",
          "shell.execute_reply.started": "2021-09-18T09:21:35.717897Z",
          "shell.execute_reply": "2021-09-18T09:21:35.729549Z"
        },
        "trusted": true,
        "outputId": "2b394137-1084-42c6-8457-a654e5f3b887"
      },
      "source": [
        "max_len_arg = 55\n",
        "max_len_kp = 32\n",
        "max_len_topic = 12\n",
        "max_len_sent_1_sts = 58\n",
        "max_len_sent_2_sts = 65\n",
        "median_len_sent_1_sts = 9\n",
        "median_len_sent_2_sts = 9\n",
        "max_noun_encoded_feature_len = 66\n",
        "max_dependency_encoded_feature_len = 66\n",
        "max_len_input = 128\n",
        "model_with_no_token_types =['roberta', 'bart' ,'distilbert','deberta', 'xlmroberta', 'xlnet','xlnetlarge', 'robertalarge', 'bartlarge','debertalarge','xlmrobertalarge','albertlarge']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\nException in thread Thread-4:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 522, in _execute_operations\n    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 200, in response\n    swagger_result = self._get_swagger_result(incoming_response)\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 124, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 303, in _get_swagger_result\n    self.request_config.response_callbacks,\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 353, in unmarshal_response\n    raise_on_expected(incoming_response)\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 422, in raise_on_expected\n    swagger_result=http_response.swagger_result)\nbravado.exception.HTTPUnprocessableEntity: 422 \n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/threading/daemon.py\", line 54, in run\n    self.work()\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 177, in work\n    self.process_batch(batch, version)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/threading/daemon.py\", line 78, in wrapper\n    result = func(self_, *args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 187, in process_batch\n    result = self._processor._backend.execute_operations(self._processor._run_id, batch)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 412, in execute_operations\n    errors.extend(self._execute_operations(run_id, other_operations))\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/utils.py\", line 62, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 527, in _execute_operations\n    raise NeptuneStorageLimitException()\nneptune.new.exceptions.NeptuneStorageLimitException: \n\u001b[95m\n----NeptuneStorageLimitException---------------------------------------------------------------------------------------\n\u001b[0m\nYou exceeded storage limit for workspace. It's not possible to upload new data, but you can still fetch and delete data.\nIf you are using asynchronous (default) connection mode Neptune automatically switched to an offline mode\nand your data is being stored safely on the disk. You can upload it later using Neptune Command Line Interface:\n    \u001b[95mneptune sync -p project_name\u001b[0m\nWhat should I do?\n    - Go to your projects and remove runs or model metadata you don't need\n    - ... or update your subscription plan here: https://app.neptune.ai/-/subscription\nYou may also want to check the following docs pages:\n    - https://docs.neptune.ai/advanced-user-guides/connection-modes\n\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n\nWarning: string series 'monitoring/stderr' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGqp-fEapGS"
      },
      "source": [
        "### Function to make TensorDataset of the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPHZFv_RUTYL",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:35.734499Z",
          "iopub.execute_input": "2021-09-18T09:21:35.734825Z",
          "iopub.status.idle": "2021-09-18T09:21:35.752963Z",
          "shell.execute_reply.started": "2021-09-18T09:21:35.734776Z",
          "shell.execute_reply": "2021-09-18T09:21:35.752115Z"
        },
        "trusted": true
      },
      "source": [
        "def make_dataset(tokenizer, args,kps,topics,features, labels, max_len_input, model_with_no_token_types = model_with_no_token_types, model_name='roberta'):\n",
        "    \n",
        "    all_input_ids = []\n",
        "    all_token_type_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_labels = [] \n",
        "    all_features=[]\n",
        "    \n",
        "    for arg,kp,topic,feature,label in zip(args,kps,topics,features,labels) :\n",
        "\n",
        "        arg = re.sub('[^a-zA-Z]', ' ', arg)\n",
        "        kp = re.sub('[^a-zA-Z]', ' ', kp)\n",
        "        topic = re.sub('[^a-zA-Z]', ' ', topic)\n",
        "\n",
        "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        arg = url.sub(r'',arg)\n",
        "        kp = url.sub(r'',kp)\n",
        "        topic = url.sub(r'',topic)\n",
        "        \n",
        "        html=re.compile(r'<.*?>')\n",
        "        arg = html.sub(r'',arg)\n",
        "        kp = html.sub(r'',kp)\n",
        "        topic = html.sub(r'',topic)\n",
        "\n",
        "\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        \n",
        "        arg = emoji_pattern.sub(r'',arg)\n",
        "        kp = emoji_pattern.sub(r'',kp)\n",
        "        topic = emoji_pattern.sub(r'',topic)\n",
        "\n",
        "        if model_name in model_with_no_token_types:\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg+topic,max_length = max_len_input, padding='max_length')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          #all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_labels.append(label)\n",
        "          all_features.append(feature)\n",
        "\n",
        "        else :\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg+topic,max_length = max_len_input, padding='max_length')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_labels.append(label)\n",
        "          all_features.append(feature)\n",
        "          \n",
        "    if model_name in model_with_no_token_types:\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels)\n",
        "      \n",
        "      dataset = TensorDataset(all_input_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    else :\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_token_type_ids = torch.tensor(all_token_type_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels) \n",
        "\n",
        "      dataset = TensorDataset(all_input_ids,all_token_type_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:35.756105Z",
          "iopub.execute_input": "2021-09-18T09:21:35.756656Z",
          "iopub.status.idle": "2021-09-18T09:21:35.771270Z",
          "shell.execute_reply.started": "2021-09-18T09:21:35.756617Z",
          "shell.execute_reply": "2021-09-18T09:21:35.770459Z"
        },
        "trusted": true,
        "id": "bsOiL4SJSVra"
      },
      "source": [
        "def make_dataset_additional(tokenizer, sents_1,sents_2,features, labels, max_len_input, model_with_no_token_types = model_with_no_token_types, model_name='roberta'):\n",
        "    \n",
        "    all_input_ids = []\n",
        "    all_token_type_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_features=[]\n",
        "    all_labels = [] \n",
        "    \n",
        "    for arg,kp,feature,label in zip(sents_1,sents_2,features, labels) :\n",
        "\n",
        "        arg = re.sub('[^a-zA-Z]', ' ', arg)\n",
        "        kp = re.sub('[^a-zA-Z]', ' ', kp)\n",
        "        #topic = re.sub('[^a-zA-Z]', ' ', topic)\n",
        "\n",
        "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        arg = url.sub(r'',arg)\n",
        "        kp = url.sub(r'',kp)\n",
        "        #topic = url.sub(r'',topic)\n",
        "        \n",
        "        html=re.compile(r'<.*?>')\n",
        "        arg = html.sub(r'',arg)\n",
        "        kp = html.sub(r'',kp)\n",
        "        #topic = html.sub(r'',topic)\n",
        "\n",
        "\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        \n",
        "        arg = emoji_pattern.sub(r'',arg)\n",
        "        kp = emoji_pattern.sub(r'',kp)\n",
        "        #topic = emoji_pattern.sub(r'',topic)\n",
        "\n",
        "        if model_name in model_with_no_token_types:\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg,max_length = max_len_input, padding='max_length',truncation='longest_first')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          #all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_features.append(feature)\n",
        "          all_labels.append(label)\n",
        "\n",
        "        else :\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg,max_length = max_len_input, padding='max_length',truncation='longest_first')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_features.append(feature)\n",
        "          all_labels.append(label)\n",
        "          \n",
        "    if model_name in model_with_no_token_types:\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels)\n",
        "      \n",
        "      dataset = TensorDataset(all_input_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    else :\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_token_type_ids = torch.tensor(all_token_type_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels) \n",
        "\n",
        "      dataset = TensorDataset(all_input_ids,all_token_type_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9Frxkba6_2"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dfFMEJQUTV4",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:35.773568Z",
          "iopub.execute_input": "2021-09-18T09:21:35.774020Z",
          "iopub.status.idle": "2021-09-18T09:21:42.054900Z",
          "shell.execute_reply.started": "2021-09-18T09:21:35.773983Z",
          "shell.execute_reply": "2021-09-18T09:21:42.054039Z"
        },
        "trusted": true,
        "outputId": "4f96ede7-41d9-41ed-e2e0-00c66306b9f6"
      },
      "source": [
        "df_sts = pd.read_csv(path_dataset+'sts_dataset.csv')\n",
        "df_arg30 = pd.read_csv(path_dataset+'30k_dataset.csv')\n",
        "df_train = pd.read_csv(path_dataset+'train_tfidf.csv')\n",
        "df_val = pd.read_csv(path_dataset+'val_tfidf.csv')\n",
        "df_test  = pd.read_csv(path_dataset+'final_test.csv')\n",
        "\n",
        "print(df_train.shape,df_val.shape,df_test.shape,df_sts.shape,df_arg30.shape)\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(20635, 31) (3458, 31) (3923, 18) (8020, 19) (30497, 18)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n0           0             0               0                 0   \n1           1             1               1                 1   \n2           2             2               2                 2   \n3           3             3               3                 3   \n4           4             4               4                 4   \n\n   Unnamed: 0.1.1.1.1  Unnamed: 0.1.1.1.1.1     arg_id key_point_id  label  \\\n0                   0                     0    arg_0_0       kp_0_0      0   \n1                   1                     1  arg_0_121       kp_0_4      0   \n2                   2                     2  arg_0_121       kp_0_5      0   \n3                   3                     3  arg_0_121       kp_0_6      1   \n4                   4                     4  arg_0_121       kp_0_7      0   \n\n                                                 arg  ...  \\\n0  `people reach their limit when it comes to the...  ...   \n1  a cure or treatment may be discovered shortly ...  ...   \n2  a cure or treatment may be discovered shortly ...  ...   \n3  a cure or treatment may be discovered shortly ...  ...   \n4  a cure or treatment may be discovered shortly ...  ...   \n\n                         encoded_dependency_features  \\\n0  [1, 2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 4, 11, 12, ...   \n1  [1, 2, 3, 2, 9, 22, 4, 9, 8, 15, 12, 1, 23, 7,...   \n2  [1, 2, 3, 17, 15, 24, 23, 4, 16, 11, 9, 20, 11...   \n3  [1, 19, 9, 26, 20, 3, 27, 1, 28, 2, 9, 13, 14,...   \n4  [1, 2, 3, 6, 12, 25, 23, 19, 16, 11, 9, 20, 22...   \n\n                               encoded_noun_features  \\\n0  [1, 2, 2, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, ...   \n1  [1, 2, 2, 2, 2, 2, 2, 2, 7, 2, 1, 1, 1, 3, 1, ...   \n2  [1, 4, 4, 4, 5, 3, 1, 2, 1, 1, 2, 2, 4, 3, 2, ...   \n3  [1, 2, 2, 2, 2, 2, 4, 1, 4, 4, 4, 4, 2, 1, 4, ...   \n4  [1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 3, 2, ...   \n\n   encoded_noun_features_len encoded_dependency_features_len  \\\n0                         60                              60   \n1                         35                              35   \n2                         29                              29   \n3                         36                              36   \n4                         29                              29   \n\n                                     tf_idf_features  tf_idf_len  \\\n0  [0.5073060076370642, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31489661...         298   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n\n                              tf_idf_bigram_features  tf_idf_bigram_len  \\\n0  [0.419428095234635, 0.0, 0.0, 0.0, 0.0, 0.0, 0...                329   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n\n                             tf_idf_trigram_features tf_idf_trigram_len  \n0  [0.38897968136796685, 0.0, 0.0, 0.0, 0.0, 0.0,...                684  \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n\n[5 rows x 31 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0.1.1</th>\n      <th>Unnamed: 0.1.1.1</th>\n      <th>Unnamed: 0.1.1.1.1</th>\n      <th>Unnamed: 0.1.1.1.1.1</th>\n      <th>arg_id</th>\n      <th>key_point_id</th>\n      <th>label</th>\n      <th>arg</th>\n      <th>...</th>\n      <th>encoded_dependency_features</th>\n      <th>encoded_noun_features</th>\n      <th>encoded_noun_features_len</th>\n      <th>encoded_dependency_features_len</th>\n      <th>tf_idf_features</th>\n      <th>tf_idf_len</th>\n      <th>tf_idf_bigram_features</th>\n      <th>tf_idf_bigram_len</th>\n      <th>tf_idf_trigram_features</th>\n      <th>tf_idf_trigram_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>arg_0_0</td>\n      <td>kp_0_0</td>\n      <td>0</td>\n      <td>`people reach their limit when it comes to the...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 4, 11, 12, ...</td>\n      <td>[1, 2, 2, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, ...</td>\n      <td>60</td>\n      <td>60</td>\n      <td>[0.5073060076370642, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.419428095234635, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n      <td>329</td>\n      <td>[0.38897968136796685, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>arg_0_121</td>\n      <td>kp_0_4</td>\n      <td>0</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 2, 9, 22, 4, 9, 8, 15, 12, 1, 23, 7,...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 7, 2, 1, 1, 1, 3, 1, ...</td>\n      <td>35</td>\n      <td>35</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>arg_0_121</td>\n      <td>kp_0_5</td>\n      <td>0</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 17, 15, 24, 23, 4, 16, 11, 9, 20, 11...</td>\n      <td>[1, 4, 4, 4, 5, 3, 1, 2, 1, 1, 2, 2, 4, 3, 2, ...</td>\n      <td>29</td>\n      <td>29</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31489661...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>arg_0_121</td>\n      <td>kp_0_6</td>\n      <td>1</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 19, 9, 26, 20, 3, 27, 1, 28, 2, 9, 13, 14,...</td>\n      <td>[1, 2, 2, 2, 2, 2, 4, 1, 4, 4, 4, 4, 2, 1, 4, ...</td>\n      <td>36</td>\n      <td>36</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>arg_0_121</td>\n      <td>kp_0_7</td>\n      <td>0</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 6, 12, 25, 23, 19, 16, 11, 9, 20, 22...</td>\n      <td>[1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 3, 2, ...</td>\n      <td>29</td>\n      <td>29</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.056197Z",
          "iopub.execute_input": "2021-09-18T09:21:42.056543Z",
          "iopub.status.idle": "2021-09-18T09:21:42.067860Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.056507Z",
          "shell.execute_reply": "2021-09-18T09:21:42.066767Z"
        },
        "trusted": true,
        "id": "rF_sl8SqSVrc",
        "outputId": "988f8c2b-d376-44f2-bea0-c47cb043c8f6"
      },
      "source": [
        "max(df_train['encoded_noun_features_len']),max(df_val['encoded_noun_features_len']),max(df_test['encoded_noun_features_len'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(66, 60, 60)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.069569Z",
          "iopub.execute_input": "2021-09-18T09:21:42.070302Z",
          "iopub.status.idle": "2021-09-18T09:21:42.082244Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.070263Z",
          "shell.execute_reply": "2021-09-18T09:21:42.080986Z"
        },
        "trusted": true,
        "id": "5wjlA1JmSVrc",
        "outputId": "d15c1120-e9ae-4ed8-e4f6-e6c1be6d5c41"
      },
      "source": [
        "max(df_train['encoded_dependency_features_len']),max(df_val['encoded_dependency_features_len']),max(df_test['encoded_dependency_features_len'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(66, 60, 60)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekvTJTEeUTTZ",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.083869Z",
          "iopub.execute_input": "2021-09-18T09:21:42.084309Z",
          "iopub.status.idle": "2021-09-18T09:21:42.108643Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.084274Z",
          "shell.execute_reply": "2021-09-18T09:21:42.107800Z"
        },
        "trusted": true,
        "outputId": "4f3286da-986f-4c7c-ad29-92c3165671f4"
      },
      "source": [
        "df_sts.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n0           0             0               0                 0   \n1           1             1               1                 1   \n2           2             2               2                 2   \n3           3             3               3                 3   \n4           4             4               4                 4   \n\n                                          sent_1  \\\n0                         A plane is taking off.   \n1                A man is playing a large flute.   \n2  A man is spreading shreded cheese on a pizza.   \n3                   Three men are playing chess.   \n4                    A man is playing the cello.   \n\n                                              sent_2  label  label_normalized  \\\n0                        An air plane is taking off.   5.00              1.00   \n1                          A man is playing a flute.   3.80              0.76   \n2  A man is spreading shredded cheese on an uncoo...   3.80              0.76   \n3                         Two men are playing chess.   2.60              0.52   \n4                 A man seated is playing the cello.   4.25              0.85   \n\n   sent_1_token_lengths  sent_2_token_lengths  \\\n0                     6                     7   \n1                     9                     8   \n2                    11                    12   \n3                     6                     6   \n4                     8                     9   \n\n                                      input_sentence  \\\n0  A plane is taking off.An air plane is taking off.   \n1  A man is playing a large flute.A man is playin...   \n2  A man is spreading shreded cheese on a pizza.A...   \n3  Three men are playing chess.Two men are playin...   \n4  A man is playing the cello.A man seated is pla...   \n\n                                 dependency_features  \\\n0  ['det', 'nsubj', 'aux', 'ROOT', 'prt', 'punct'...   \n1  ['det', 'nsubj', 'aux', 'ROOT', 'det', 'amod',...   \n2  ['det', 'nsubj', 'aux', 'ROOT', 'amod', 'dobj'...   \n3  ['nummod', 'nsubj', 'aux', 'ROOT', 'dobj', 'pu...   \n4  ['det', 'nsubj', 'aux', 'ROOT', 'det', 'dobj',...   \n\n                                       noun_features  \\\n0  ['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...   \n1  ['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'NOUN...   \n2  ['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...   \n3  ['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...   \n4  ['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...   \n\n                         encoded_dependency_features  \\\n0  [1, 2, 3, 4, 5, 6, 1, 7, 2, 3, 4, 5, 6, 0, 0, ...   \n1  [1, 2, 3, 4, 1, 8, 9, 6, 1, 2, 3, 4, 1, 9, 6, ...   \n2  [1, 2, 3, 4, 8, 9, 10, 1, 11, 6, 1, 2, 3, 4, 8...   \n3  [12, 2, 3, 4, 9, 6, 12, 2, 3, 4, 9, 6, 0, 0, 0...   \n4  [1, 2, 3, 4, 1, 9, 6, 1, 2, 13, 3, 4, 1, 9, 6,...   \n\n                               encoded_noun_features  \\\n0  [1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, ...   \n1  [1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, ...   \n2  [1, 2, 2, 2, 1, 2, 2, 1, 3, 2, 1, 2, 2, 2, 1, ...   \n3  [1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, ...   \n4  [1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, ...   \n\n   encoded_noun_features_len  encoded_dependency_features_len  \\\n0                         84                               84   \n1                         84                               84   \n2                         84                               84   \n3                         84                               84   \n4                         84                               84   \n\n                              tf_idf_bigram_features  tf_idf_bigram_len  \n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                615  \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                615  \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                615  \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                615  \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                615  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0.1.1</th>\n      <th>Unnamed: 0.1.1.1</th>\n      <th>sent_1</th>\n      <th>sent_2</th>\n      <th>label</th>\n      <th>label_normalized</th>\n      <th>sent_1_token_lengths</th>\n      <th>sent_2_token_lengths</th>\n      <th>input_sentence</th>\n      <th>dependency_features</th>\n      <th>noun_features</th>\n      <th>encoded_dependency_features</th>\n      <th>encoded_noun_features</th>\n      <th>encoded_noun_features_len</th>\n      <th>encoded_dependency_features_len</th>\n      <th>tf_idf_bigram_features</th>\n      <th>tf_idf_bigram_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>A plane is taking off.</td>\n      <td>An air plane is taking off.</td>\n      <td>5.00</td>\n      <td>1.00</td>\n      <td>6</td>\n      <td>7</td>\n      <td>A plane is taking off.An air plane is taking off.</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'prt', 'punct'...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...</td>\n      <td>[1, 2, 3, 4, 5, 6, 1, 7, 2, 3, 4, 5, 6, 0, 0, ...</td>\n      <td>[1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, ...</td>\n      <td>84</td>\n      <td>84</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>A man is playing a large flute.</td>\n      <td>A man is playing a flute.</td>\n      <td>3.80</td>\n      <td>0.76</td>\n      <td>9</td>\n      <td>8</td>\n      <td>A man is playing a large flute.A man is playin...</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'det', 'amod',...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'NOUN...</td>\n      <td>[1, 2, 3, 4, 1, 8, 9, 6, 1, 2, 3, 4, 1, 9, 6, ...</td>\n      <td>[1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, ...</td>\n      <td>84</td>\n      <td>84</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>A man is spreading shreded cheese on a pizza.</td>\n      <td>A man is spreading shredded cheese on an uncoo...</td>\n      <td>3.80</td>\n      <td>0.76</td>\n      <td>11</td>\n      <td>12</td>\n      <td>A man is spreading shreded cheese on a pizza.A...</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'amod', 'dobj'...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...</td>\n      <td>[1, 2, 3, 4, 8, 9, 10, 1, 11, 6, 1, 2, 3, 4, 8...</td>\n      <td>[1, 2, 2, 2, 1, 2, 2, 1, 3, 2, 1, 2, 2, 2, 1, ...</td>\n      <td>84</td>\n      <td>84</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>Three men are playing chess.</td>\n      <td>Two men are playing chess.</td>\n      <td>2.60</td>\n      <td>0.52</td>\n      <td>6</td>\n      <td>6</td>\n      <td>Three men are playing chess.Two men are playin...</td>\n      <td>['nummod', 'nsubj', 'aux', 'ROOT', 'dobj', 'pu...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...</td>\n      <td>[12, 2, 3, 4, 9, 6, 12, 2, 3, 4, 9, 6, 0, 0, 0...</td>\n      <td>[1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, ...</td>\n      <td>84</td>\n      <td>84</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>615</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>A man is playing the cello.</td>\n      <td>A man seated is playing the cello.</td>\n      <td>4.25</td>\n      <td>0.85</td>\n      <td>8</td>\n      <td>9</td>\n      <td>A man is playing the cello.A man seated is pla...</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'det', 'dobj',...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...</td>\n      <td>[1, 2, 3, 4, 1, 9, 6, 1, 2, 13, 3, 4, 1, 9, 6,...</td>\n      <td>[1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, ...</td>\n      <td>84</td>\n      <td>84</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>615</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBXpAIAfUTRB",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.109863Z",
          "iopub.execute_input": "2021-09-18T09:21:42.110223Z",
          "iopub.status.idle": "2021-09-18T09:21:42.130368Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.110187Z",
          "shell.execute_reply": "2021-09-18T09:21:42.129222Z"
        },
        "trusted": true,
        "outputId": "d94b6368-ed0f-4070-9a5b-08b7cedb89f3"
      },
      "source": [
        "df_arg30.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0  Unnamed: 0.1  \\\n0           0             0   \n1           1             1   \n2           2             2   \n3           3             3   \n4           4             4   \n\n                                                 arg  \\\n0  \"marriage\" isn't keeping up with the times.  a...   \n1  .a multi-party system would be too confusing a...   \n2  \\ero-tolerance policy in schools should not be...   \n3  `people reach their limit when it comes to the...   \n4  100% agree, should they do that, it would be a...   \n\n                                               topic    set        WA  \\\n0                         We should abandon marriage  train  0.846165   \n1               We should adopt a multi-party system  train  0.891271   \n2  We should adopt a zero-tolerance policy in sch...    dev  0.721192   \n3      Assisted suicide should be a criminal offence  train  0.730395   \n4                      We should abolish safe spaces  train  0.236686   \n\n      label  stance_WA  stance_WA_conf  \\\n0  0.297659          1        1.000000   \n1  0.726133         -1        1.000000   \n2  0.396953         -1        1.000000   \n3  0.225212         -1        1.000000   \n4  0.004104          1        0.805517   \n\n                                      input_sentence  \\\n0  \"marriage\" isn't keeping up with the times.  a...   \n1  .a multi-party system would be too confusing a...   \n2  \\ero-tolerance policy in schools should not be...   \n3  `people reach their limit when it comes to the...   \n4  100% agree, should they do that, it would be a...   \n\n                                 dependency_features  \\\n0  ['punct', 'nsubj', 'punct', 'aux', 'neg', 'ROO...   \n1  ['punct', 'dep', 'dep', 'compound', 'nsubj', '...   \n2  ['nummod', 'punct', 'compound', 'nsubjpass', '...   \n3  ['punct', 'nsubj', 'ROOT', 'poss', 'dobj', 'ad...   \n4  ['nummod', 'nsubj', 'ccomp', 'punct', 'aux', '...   \n\n                                       noun_features  \\\n0  ['NOUN', 'VERB', 'NOUN', 'VERB', 'VERB', 'VERB...   \n1  ['AUX', 'ADJ', 'ADJ', 'NOUN', 'AUX', 'AUX', 'A...   \n2  ['NOUN', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADP'...   \n3  ['VERB', 'VERB', 'VERB', 'NOUN', 'VERB', 'VERB...   \n4  ['NOUN', 'NOUN', 'AUX', 'AUX', 'AUX', 'AUX', '...   \n\n                         encoded_dependency_features  \\\n0  [1, 2, 1, 3, 4, 5, 6, 7, 8, 9, 1, 10, 5, 8, 11...   \n1  [1, 18, 18, 19, 2, 3, 5, 16, 20, 13, 14, 8, 12...   \n2  [21, 1, 19, 22, 7, 9, 3, 4, 23, 5, 24, 2, 25, ...   \n3  [1, 2, 5, 27, 12, 16, 2, 25, 7, 27, 9, 7, 9, 1...   \n4  [21, 2, 31, 1, 3, 2, 31, 12, 1, 2, 3, 5, 8, 11...   \n\n                               encoded_noun_features  \\\n0  [1, 2, 1, 2, 2, 2, 2, 2, 1, 3, 2, 4, 2, 1, 1, ...   \n1  [7, 8, 8, 1, 7, 7, 7, 8, 7, 8, 8, 1, 2, 1, 1, ...   \n2  [1, 1, 1, 2, 1, 3, 2, 2, 2, 2, 7, 7, 2, 7, 7, ...   \n3  [2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 3, 1, 3, 2, 7, ...   \n4  [1, 1, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 1, 1, 7, ...   \n\n   encoded_noun_features_len  encoded_dependency_features_len  \\\n0                         37                               37   \n1                         29                               29   \n2                         46                               46   \n3                         52                               52   \n4                         19                               19   \n\n                              tf_idf_bigram_features  tf_idf_bigram_len  \n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                 65  \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.722679695981115, 0...                 65  \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                 65  \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484...                 65  \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                 65  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>arg</th>\n      <th>topic</th>\n      <th>set</th>\n      <th>WA</th>\n      <th>label</th>\n      <th>stance_WA</th>\n      <th>stance_WA_conf</th>\n      <th>input_sentence</th>\n      <th>dependency_features</th>\n      <th>noun_features</th>\n      <th>encoded_dependency_features</th>\n      <th>encoded_noun_features</th>\n      <th>encoded_noun_features_len</th>\n      <th>encoded_dependency_features_len</th>\n      <th>tf_idf_bigram_features</th>\n      <th>tf_idf_bigram_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n      <td>We should abandon marriage</td>\n      <td>train</td>\n      <td>0.846165</td>\n      <td>0.297659</td>\n      <td>1</td>\n      <td>1.000000</td>\n      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n      <td>['punct', 'nsubj', 'punct', 'aux', 'neg', 'ROO...</td>\n      <td>['NOUN', 'VERB', 'NOUN', 'VERB', 'VERB', 'VERB...</td>\n      <td>[1, 2, 1, 3, 4, 5, 6, 7, 8, 9, 1, 10, 5, 8, 11...</td>\n      <td>[1, 2, 1, 2, 2, 2, 2, 2, 1, 3, 2, 4, 2, 1, 1, ...</td>\n      <td>37</td>\n      <td>37</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>.a multi-party system would be too confusing a...</td>\n      <td>We should adopt a multi-party system</td>\n      <td>train</td>\n      <td>0.891271</td>\n      <td>0.726133</td>\n      <td>-1</td>\n      <td>1.000000</td>\n      <td>.a multi-party system would be too confusing a...</td>\n      <td>['punct', 'dep', 'dep', 'compound', 'nsubj', '...</td>\n      <td>['AUX', 'ADJ', 'ADJ', 'NOUN', 'AUX', 'AUX', 'A...</td>\n      <td>[1, 18, 18, 19, 2, 3, 5, 16, 20, 13, 14, 8, 12...</td>\n      <td>[7, 8, 8, 1, 7, 7, 7, 8, 7, 8, 8, 1, 2, 1, 1, ...</td>\n      <td>29</td>\n      <td>29</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.722679695981115, 0...</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>\\ero-tolerance policy in schools should not be...</td>\n      <td>We should adopt a zero-tolerance policy in sch...</td>\n      <td>dev</td>\n      <td>0.721192</td>\n      <td>0.396953</td>\n      <td>-1</td>\n      <td>1.000000</td>\n      <td>\\ero-tolerance policy in schools should not be...</td>\n      <td>['nummod', 'punct', 'compound', 'nsubjpass', '...</td>\n      <td>['NOUN', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADP'...</td>\n      <td>[21, 1, 19, 22, 7, 9, 3, 4, 23, 5, 24, 2, 25, ...</td>\n      <td>[1, 1, 1, 2, 1, 3, 2, 2, 2, 2, 7, 7, 2, 7, 7, ...</td>\n      <td>46</td>\n      <td>46</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>`people reach their limit when it comes to the...</td>\n      <td>Assisted suicide should be a criminal offence</td>\n      <td>train</td>\n      <td>0.730395</td>\n      <td>0.225212</td>\n      <td>-1</td>\n      <td>1.000000</td>\n      <td>`people reach their limit when it comes to the...</td>\n      <td>['punct', 'nsubj', 'ROOT', 'poss', 'dobj', 'ad...</td>\n      <td>['VERB', 'VERB', 'VERB', 'NOUN', 'VERB', 'VERB...</td>\n      <td>[1, 2, 5, 27, 12, 16, 2, 25, 7, 27, 9, 7, 9, 1...</td>\n      <td>[2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 3, 1, 3, 2, 7, ...</td>\n      <td>52</td>\n      <td>52</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.484...</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>100% agree, should they do that, it would be a...</td>\n      <td>We should abolish safe spaces</td>\n      <td>train</td>\n      <td>0.236686</td>\n      <td>0.004104</td>\n      <td>1</td>\n      <td>0.805517</td>\n      <td>100% agree, should they do that, it would be a...</td>\n      <td>['nummod', 'nsubj', 'ccomp', 'punct', 'aux', '...</td>\n      <td>['NOUN', 'NOUN', 'AUX', 'AUX', 'AUX', 'AUX', '...</td>\n      <td>[21, 2, 31, 1, 3, 2, 31, 12, 1, 2, 3, 5, 8, 11...</td>\n      <td>[1, 1, 7, 7, 7, 7, 1, 7, 7, 7, 7, 7, 1, 1, 7, ...</td>\n      <td>19</td>\n      <td>19</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz6pmKVqUTOi",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.131989Z",
          "iopub.execute_input": "2021-09-18T09:21:42.132715Z",
          "iopub.status.idle": "2021-09-18T09:21:42.167121Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.132672Z",
          "shell.execute_reply": "2021-09-18T09:21:42.166061Z"
        },
        "trusted": true,
        "outputId": "de415c12-ccb1-4d10-818a-4557a4fcbadf"
      },
      "source": [
        "# Here I am concatenating train and dev set for training the model for creating test dataset predictions\n",
        "df_train = pd.concat([df_train,df_val])\n",
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(24093, 31)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAKOZ8-bMUf",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.168521Z",
          "iopub.execute_input": "2021-09-18T09:21:42.168881Z",
          "iopub.status.idle": "2021-09-18T09:21:42.173306Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.168845Z",
          "shell.execute_reply": "2021-09-18T09:21:42.172364Z"
        },
        "trusted": true
      },
      "source": [
        "def give_list(sent):\n",
        "  res = ast.literal_eval(sent)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsgumEs9bMR5",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:21:42.174783Z",
          "iopub.execute_input": "2021-09-18T09:21:42.175212Z",
          "iopub.status.idle": "2021-09-18T09:22:18.334697Z",
          "shell.execute_reply.started": "2021-09-18T09:21:42.175177Z",
          "shell.execute_reply": "2021-09-18T09:22:18.333553Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "6aa9aaee6a404733beda884f247be10f",
            "9d8fd0dba7e64fd88fa73d16d47c9d36",
            "d6a80a4ca6834d0db53470ac4dd9e6ef",
            "ed797347dd63450e8ebcfaf7f2dcee3b"
          ]
        },
        "outputId": "7bee452c-ae99-4314-ac6b-723d15fe8680"
      },
      "source": [
        "## For using noun features use the first three lines of code \n",
        "## For using dependency features use the last three lines of code and comment the other three accordingly\n",
        "\n",
        "#df_train['encoded_noun_features']= df_train['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_val['encoded_noun_features']= df_val['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_test['encoded_noun_features']= df_test['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_arg30['encoded_noun_features']= df_arg30['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "\n",
        "#df_arg30['encoded_dependency_features']= df_arg30['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_train['encoded_dependency_features']= df_train['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_val['encoded_dependency_features']= df_val['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_test['encoded_dependency_features']= df_test['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))\n",
        "\n",
        "df_sts['tf_idf_bigram_features']= df_sts['tf_idf_bigram_features'].parallel_apply(lambda x: give_list(x))\n",
        "df_train['tf_idf_bigram_features']= df_train['tf_idf_bigram_features'].parallel_apply(lambda x: give_list(x))\n",
        "df_val['tf_idf_bigram_features']= df_val['tf_idf_bigram_features'].parallel_apply(lambda x: give_list(x))\n",
        "df_test['tf_idf_bigram_features']= df_test['tf_idf_bigram_features'].parallel_apply(lambda x: give_list(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4010), Label(value='0 / 4010'))), â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa9aaee6a404733beda884f247be10f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=12047), Label(value='0 / 12047')))â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d8fd0dba7e64fd88fa73d16d47c9d36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1729), Label(value='0 / 1729'))), â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6a80a4ca6834d0db53470ac4dd9e6ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1962), Label(value='0 / 1962'))), â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed797347dd63450e8ebcfaf7f2dcee3b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhIbcJzbD6E"
      },
      "source": [
        "types_of_models= model : tokenizer model_path\n",
        " - 'bert':  'bert-base-uncased'\n",
        " - 'roberta':  'roberta-base'\n",
        " - 'bart':  \"facebook/bart-base\"\n",
        " - 'distilbert': 'distilbert-base-uncased'\n",
        " - 'deberta': 'microsoft/deberta-base'\n",
        " - 'debertalarge': 'microsoft/deberta-large'\n",
        " - 'xlnet' : 'xlnet-base-cased'\n",
        " - 'xlnetlarge' : 'xlnet-large-cased'\n",
        " - 'xlmrobertalarge' : 'xlm-roberta-large'\n",
        " - 'bartlarge' : 'facebook/bart-large'\n",
        " - 'bertlarge':  'bert-large-uncased'\n",
        " - 'robertalarge':  'roberta-large'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vh9cVerUTMS",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:22:18.336283Z",
          "iopub.execute_input": "2021-09-18T09:22:18.336613Z",
          "iopub.status.idle": "2021-09-18T09:22:18.344353Z",
          "shell.execute_reply.started": "2021-09-18T09:22:18.336573Z",
          "shell.execute_reply": "2021-09-18T09:22:18.343418Z"
        },
        "trusted": true
      },
      "source": [
        "model_name = 'debertalarge'\n",
        "model_path = 'microsoft/deberta-large'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcdpBLs4UTJ6",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:22:18.345690Z",
          "iopub.execute_input": "2021-09-18T09:22:18.346394Z",
          "iopub.status.idle": "2021-09-18T09:22:24.508749Z",
          "shell.execute_reply.started": "2021-09-18T09:22:18.346352Z",
          "shell.execute_reply": "2021-09-18T09:22:24.507870Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "5e6f5a89c0bf4c83b8f10872cb5729fa",
            "3f51797c87c7404a9b78577f29d588b3",
            "b2dc3bbd88c34bf48ad970f3fa2ae971",
            "530fd8ed39be4ab89881fbfcae275fba"
          ]
        },
        "outputId": "9ac507b2-0b59-4e1b-8be0-7ed893d338b7"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/475 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e6f5a89c0bf4c83b8f10872cb5729fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f51797c87c7404a9b78577f29d588b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2dc3bbd88c34bf48ad970f3fa2ae971"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "530fd8ed39be4ab89881fbfcae275fba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tp5FYy4UTHX",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:22:24.510140Z",
          "iopub.execute_input": "2021-09-18T09:22:24.510505Z",
          "iopub.status.idle": "2021-09-18T09:22:43.220085Z",
          "shell.execute_reply.started": "2021-09-18T09:22:24.510464Z",
          "shell.execute_reply": "2021-09-18T09:22:43.219098Z"
        },
        "trusted": true
      },
      "source": [
        "## For using noun features use the first three lines of code \n",
        "## For using dependency features use the last three lines of code and comment the other three accordingly\n",
        "\n",
        "\n",
        "#train_dataset = make_dataset(tokenizer, df_train['arg'], df_train['key_point'], df_train['topic'],df_train['encoded_noun_features'], df_train['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "#val_dataset = make_dataset(tokenizer, df_val['arg'], df_val['key_point'], df_val['topic'],df_val['encoded_noun_features'], df_val['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "#test_dataset = make_dataset(tokenizer, df_test['arg'], df_test['key_point'], df_test['topic'],df_test['encoded_noun_features'], df_test['stance'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "\n",
        "sts_train_dataset = make_dataset_additional(tokenizer, df_sts['sent_1'],df_sts['sent_2'],df_sts['tf_idf_bigram_features'], df_sts['label_normalized'], max_len_input,model_with_no_token_types, model_name=model_name)\n",
        "train_dataset = make_dataset(tokenizer, df_train['arg'], df_train['key_point'], df_train['topic'],df_train['tf_idf_bigram_features'], df_train['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "val_dataset = make_dataset(tokenizer, df_val['arg'], df_val['key_point'], df_val['topic'],df_val['tf_idf_bigram_features'], df_val['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "test_dataset = make_dataset(tokenizer, df_test['arg'], df_test['key_point'], df_test['topic'],df_test['tf_idf_bigram_features'], df_test['stance'], max_len_input, model_with_no_token_types, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyrsI4_RbcWp"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLb0hjRLUTEr",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:22:43.221491Z",
          "iopub.execute_input": "2021-09-18T09:22:43.221848Z",
          "iopub.status.idle": "2021-09-18T09:22:43.231184Z",
          "shell.execute_reply.started": "2021-09-18T09:22:43.221811Z",
          "shell.execute_reply": "2021-09-18T09:22:43.230132Z"
        },
        "trusted": true
      },
      "source": [
        "# Use this Class only for Bert base and Bert large model\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        #Instantiating Pre trained model object \n",
        "        self.model_layer = AutoModel.from_pretrained(model_path)\n",
        "        \n",
        "        #Layers\n",
        "        # the first dense layer will have 834 neurons if base model is used and \n",
        "        # 1090 neurons if large model is used\n",
        "\n",
        "        self.dense_layer_1 = nn.Linear(1090, 256)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.dense_layer_2 = nn.Linear(256, 128)\n",
        "        self.dropout_2 = nn.Dropout(0.4) \n",
        "        self.cls_layer = nn.Linear(128, 1, bias = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,input_ids, attention_masks, token_type_ids, features):\n",
        "\n",
        "        pooled_output = self.model_layer(input_ids=input_ids, attention_mask=attention_masks,token_type_ids = token_type_ids).pooler_output\n",
        "\n",
        "        ## Combining the noun features and bert pooler output\n",
        "        concat = torch.cat((pooled_output,features),dim =1)\n",
        "        \n",
        "        x = self.dense_layer_1(concat)\n",
        "        x = self.dropout(x)\n",
        "        x_1 = self.dense_layer_2(x)\n",
        "        x_2 = self.dropout_2(x_1)\n",
        "        \n",
        "        logits = self.cls_layer(x_2)\n",
        "        output = self.sigmoid(logits)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZAK_TSoUTCD",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:22:43.232791Z",
          "iopub.execute_input": "2021-09-18T09:22:43.233424Z",
          "iopub.status.idle": "2021-09-18T09:22:43.245930Z",
          "shell.execute_reply.started": "2021-09-18T09:22:43.233382Z",
          "shell.execute_reply": "2021-09-18T09:22:43.244995Z"
        },
        "trusted": true
      },
      "source": [
        "# Use this Class for the rest of transformer models\n",
        "\n",
        "class NonPoolerTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NonPoolerTransformer, self).__init__()\n",
        "        \n",
        "        #Instantiating Pre trained model object \n",
        "        self.model_layer = AutoModel.from_pretrained(model_path)\n",
        "        \n",
        "        #Layers\n",
        "        # the first dense layer will have 834 neurons if base model is used and \n",
        "        # 1090 neurons if large model is used\n",
        "\n",
        "        self.dense_layer_1 = nn.Linear(1529, 256)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.dense_layer_2 = nn.Linear(256, 128)\n",
        "        self.dropout_2 = nn.Dropout(0.4)\n",
        "        self.cls_layer = nn.Linear(128, 1, bias = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,input_ids, attention_masks,features):\n",
        "\n",
        "        hidden_state = self.model_layer(input_ids=input_ids, attention_mask=attention_masks)[0]\n",
        "        pooled_output = hidden_state[:, 0]\n",
        "\n",
        "        ## Combining the noun features and model pooler output\n",
        "        concat = torch.cat((pooled_output,features),dim =1)\n",
        "\n",
        "        x = self.dense_layer_1(concat)\n",
        "        x = self.dropout(x)\n",
        "        x_1 = self.dense_layer_2(x)\n",
        "        x_2 = self.dropout_2(x_1)\n",
        "        \n",
        "        logits = self.cls_layer(x_2)\n",
        "        output = self.sigmoid(logits)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XpYQFpOUS_s",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:22:43.248380Z",
          "iopub.execute_input": "2021-09-18T09:22:43.249756Z",
          "iopub.status.idle": "2021-09-18T09:23:48.839050Z",
          "shell.execute_reply.started": "2021-09-18T09:22:43.249723Z",
          "shell.execute_reply": "2021-09-18T09:23:48.838039Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "0a19fe053e6f485aaef3e4fe00fd15e6"
          ]
        },
        "outputId": "f411f421-9b58-41d6-916c-61f160cde447"
      },
      "source": [
        "# comment the nonpoolertransformer line for bert model and the transformer model line for the rest of the transformer models\n",
        "\n",
        "model = NonPoolerTransformer()\n",
        "#model = Transformer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a19fe053e6f485aaef3e4fe00fd15e6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y06EazsOUS9B",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:48.846490Z",
          "iopub.execute_input": "2021-09-18T09:23:48.848604Z",
          "iopub.status.idle": "2021-09-18T09:23:55.019249Z",
          "shell.execute_reply.started": "2021-09-18T09:23:48.848565Z",
          "shell.execute_reply": "2021-09-18T09:23:55.018269Z"
        },
        "trusted": true
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF9w7JvlUS6k",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.021395Z",
          "iopub.execute_input": "2021-09-18T09:23:55.021776Z",
          "iopub.status.idle": "2021-09-18T09:23:55.027291Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.021736Z",
          "shell.execute_reply": "2021-09-18T09:23:55.026173Z"
        },
        "trusted": true
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 3\n",
        "ACCUMULATION_STEPS = 2\n",
        "DROPOUT = 0.4\n",
        "gold_data_dir = path_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Kz81sxUS4J",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.028949Z",
          "iopub.execute_input": "2021-09-18T09:23:55.029578Z",
          "iopub.status.idle": "2021-09-18T09:23:55.039098Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.029537Z",
          "shell.execute_reply": "2021-09-18T09:23:55.038078Z"
        },
        "trusted": true
      },
      "source": [
        "PARAMS = {'model_name': model_name,'model_path': model_path,'lr': LEARNING_RATE, 'epoch_nr': EPOCHS, 'batch_size': BATCH_SIZE, 'accumulation_steps': ACCUMULATION_STEPS,'dropout': DROPOUT}\n",
        "run['details'] = PARAMS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG21pTKUc4mj"
      },
      "source": [
        "### Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZqgD1lmUS14",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.040316Z",
          "iopub.execute_input": "2021-09-18T09:23:55.040664Z",
          "iopub.status.idle": "2021-09-18T09:23:55.047102Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.040627Z",
          "shell.execute_reply": "2021-09-18T09:23:55.045885Z"
        },
        "trusted": true
      },
      "source": [
        "def load_kpm_data(gold_data_dir, subset):\n",
        "    \n",
        "    arguments_file = os.path.join(gold_data_dir, f\"arguments_{subset}.csv\")\n",
        "    key_points_file = os.path.join(gold_data_dir, f\"key_points_{subset}.csv\")\n",
        "    labels_file = os.path.join(gold_data_dir, f\"labels_{subset}.csv\")\n",
        "\n",
        "    arguments_df = pd.read_csv(arguments_file)\n",
        "    key_points_df = pd.read_csv(key_points_file)\n",
        "    labels_file_df = pd.read_csv(labels_file)\n",
        "    \n",
        "    return arguments_df, key_points_df, labels_file_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GNshalEUSzY",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.048702Z",
          "iopub.execute_input": "2021-09-18T09:23:55.049254Z",
          "iopub.status.idle": "2021-09-18T09:23:55.061494Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.049213Z",
          "shell.execute_reply": "2021-09-18T09:23:55.060540Z"
        },
        "trusted": true
      },
      "source": [
        "def get_predictions(predictions_file, labels_df, arg_df, kp_df):\n",
        "    #print(\"\\nÖ¿** loading predictions:\")\n",
        "    arg_df = arg_df[[\"arg_id\", \"topic\", \"stance\"]]\n",
        "    predictions_df = load_predictions(predictions_file, kp_df[\"key_point_id\"].unique())\n",
        "\n",
        "    #make sure each arg_id has a prediction\n",
        "    predictions_df = pd.merge(arg_df, predictions_df, how=\"left\", on=\"arg_id\")\n",
        "\n",
        "    #handle arguements with no matching key point\n",
        "    predictions_df[\"key_point_id\"] = predictions_df[\"key_point_id\"].fillna(\"dummy_id\")\n",
        "    predictions_df[\"score\"] = predictions_df[\"score\"].fillna(0)\n",
        "\n",
        "    #merge each argument with the gold labels\n",
        "    merged_df = pd.merge(predictions_df, labels_df, how=\"left\", on=[\"arg_id\", \"key_point_id\"])\n",
        "\n",
        "    merged_df.loc[merged_df['key_point_id'] == \"dummy_id\", 'label'] = 0\n",
        "    merged_df[\"label_strict\"] = merged_df[\"label\"].fillna(0)\n",
        "    merged_df[\"label_relaxed\"] = merged_df[\"label\"].fillna(1)\n",
        "\n",
        "    return merged_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BfGWC5LUSw-",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.062846Z",
          "iopub.execute_input": "2021-09-18T09:23:55.063290Z",
          "iopub.status.idle": "2021-09-18T09:23:55.072429Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.063253Z",
          "shell.execute_reply": "2021-09-18T09:23:55.071405Z"
        },
        "trusted": true
      },
      "source": [
        "def load_predictions(predictions_dir, correct_kp_list):\n",
        "    arg =[]\n",
        "    kp = []\n",
        "    scores = []\n",
        "    invalid_keypoints = set()\n",
        "    with open(predictions_dir, \"r\") as f_in:\n",
        "        res = json.load(f_in)\n",
        "        for arg_id, kps in res.items():\n",
        "            valid_kps = {key: value for key, value in kps.items() if key in correct_kp_list}\n",
        "            invalid = {key: value for key, value in kps.items() if key not in correct_kp_list}\n",
        "            for invalid_kp, _ in invalid.items():\n",
        "                if invalid_kp not in invalid_keypoints:\n",
        "                    #print(f\"key point {invalid_kp} doesn't appear in the key points file and will be ignored\")\n",
        "                    invalid_keypoints.add(invalid_kp)\n",
        "            if valid_kps:\n",
        "                best_kp = max(valid_kps.items(), key=lambda x: x[1])\n",
        "                arg.append(arg_id)\n",
        "                kp.append(best_kp[0])\n",
        "                scores.append(best_kp[1])\n",
        "        #print(f\"\\tloaded predictions for {len(arg)} arguments\")\n",
        "        \n",
        "        return pd.DataFrame({\"arg_id\" : arg, \"key_point_id\": kp, \"score\": scores})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdXgzQn6USuh",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.074157Z",
          "iopub.execute_input": "2021-09-18T09:23:55.074744Z",
          "iopub.status.idle": "2021-09-18T09:23:55.084964Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.074641Z",
          "shell.execute_reply": "2021-09-18T09:23:55.083919Z"
        },
        "trusted": true
      },
      "source": [
        "def get_ap(df, label_column, top_percentile=0.5):\n",
        "    top = int(len(df)*top_percentile)\n",
        "    df = df.sort_values('score', ascending=False).head(top)\n",
        "    # after selecting top percentile candidates, we set the score for the dummy kp to 1, to prevent it from increasing the precision.\n",
        "    df.loc[df['key_point_id'] == \"dummy_id\", 'score'] = 0.99\n",
        "    return average_precision_score(y_true=df[label_column], y_score=df[\"score\"])\n",
        "\n",
        "def calc_mean_average_precision(df, label_column):\n",
        "    precisions = [get_ap(group, label_column) for _, group in df.groupby([\"topic\", \"stance\"])]\n",
        "    return np.mean(precisions)\n",
        "\n",
        "def evaluate_predictions(merged_df,name = 'train'):\n",
        "    #print(\"\\n** running evalution:\")\n",
        "    mAP_strict = calc_mean_average_precision(merged_df, \"label_strict\")\n",
        "    mAP_relaxed = calc_mean_average_precision(merged_df, \"label_relaxed\")\n",
        "    # below two lines are added for neptune results logging\n",
        "    run[f\"{name}/map\"].log(mAP_strict)\n",
        "    run[f\"{name}/map_relaxed\"].log(mAP_relaxed)\n",
        "                         \n",
        "    print(f\"mAP strict= {mAP_strict} ; mAP relaxed = {mAP_relaxed}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6MfrZRmdGFV"
      },
      "source": [
        "### Train and Predict Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo2ZBjKZUSsE",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.086585Z",
          "iopub.execute_input": "2021-09-18T09:23:55.086989Z",
          "iopub.status.idle": "2021-09-18T09:23:55.108299Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.086956Z",
          "shell.execute_reply": "2021-09-18T09:23:55.107372Z"
        },
        "trusted": true
      },
      "source": [
        "def evaluate_model(test_dataset,df, model,  model_name, mode = 'train'):\n",
        "    \n",
        "    save_predictions_name = model_name+ '__VAL_PREDS_'+ 'SEED_'+ str(SEED) + '_dense_layer' +'_epoc_'+ str(EPOCHS)+'_lr_'+ str(LEARNING_RATE)+'_b_s_'+ str(BATCH_SIZE) +'_accumulation_steps_'+ str(ACCUMULATION_STEPS) +'_input_type_kp_arg_topic_feature'\n",
        "\n",
        "    y_preds = []\n",
        "    val_losses = []\n",
        "    criterion = nn.BCELoss()\n",
        "    list_of_batch_losses = []\n",
        "    \n",
        "    if mode in ['train','val']:\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        acc_epoch = []\n",
        "        \n",
        "        epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.eval()\n",
        "            \n",
        "            if model_name in model_with_no_token_types:\n",
        "                b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device),batch[3].to(device)\n",
        "                ypred = model(b_input_ids, b_input_mask,b_features)\n",
        "            else:\n",
        "                b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device),batch[4].to(device)\n",
        "                ypred = model(b_input_ids, b_input_mask,b_token_type,b_features)\n",
        "                \n",
        "            b_labels_copy = torch.reshape(b_labels, (b_labels.shape[0], 1))\n",
        "            loss_batch = criterion(ypred, b_labels_copy.float())\n",
        "            list_of_batch_losses.append(loss_batch.detach().cpu().numpy())\n",
        "            run[\"val/batch_loss\"].log(np.mean(loss_batch.detach().cpu().numpy()))\n",
        "            \n",
        "            ypred = ypred.cpu().numpy()\n",
        "            b_labels = batch[-1].cpu().detach().numpy()\n",
        "        \n",
        "            ypred = np.hstack(ypred)\n",
        "            y_preds.append(ypred)\n",
        "    \n",
        "    epoch_loss = np.mean(list_of_batch_losses)\n",
        "    val_losses.append(epoch_loss)\n",
        "    run[\"val/epoch_loss\"].log(epoch_loss)\n",
        "    \n",
        "    args = df['arg_id']\n",
        "    kps = df['key_point_id']\n",
        "    true_labels = df['label']\n",
        "    topics = df['topic']\n",
        "    stances = df['stance']\n",
        "    all_preds = []\n",
        "\n",
        "    for i in tqdm(range(len(y_preds))):\n",
        "      for p in y_preds[i]:\n",
        "        all_preds.append(p)\n",
        "            \n",
        "    print('Val evaluation....')\n",
        "    \n",
        "    pred_file = pd.DataFrame({\"arg_id\" : args, \"key_point_id\": kps, \"score\": all_preds})\n",
        "    args = {}\n",
        "    kps = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg] = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg][kp] = score\n",
        "\n",
        "    with open(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', 'w') as fp:\n",
        "        fp.write(json.dumps(args))\n",
        "        fp.close()\n",
        "    \n",
        "    arg_df, kp_df, labels_df = load_kpm_data(path_dataset, subset=\"dev\")\n",
        "    merged_df = get_predictions(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', labels_df, arg_df, kp_df)\n",
        "    \n",
        "    evaluate_predictions(merged_df,name = 'val')\n",
        "\n",
        "    return all_preds,true_labels, val_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X3ATAiOUSp_",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.109742Z",
          "iopub.execute_input": "2021-09-18T09:23:55.110131Z",
          "iopub.status.idle": "2021-09-18T09:23:55.137083Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.110096Z",
          "shell.execute_reply": "2021-09-18T09:23:55.136314Z"
        },
        "trusted": true
      },
      "source": [
        "def train_and_evaluate(train_dataset,df,model, filepath, model_name, batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = EPOCHS,accumulation_steps = ACCUMULATION_STEPS):\n",
        "  \n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "    \n",
        "  save_model = model_name+ '_SEED_'+ str(SEED) +'_dense_layer' +'_epoc_'+ str(epochs)+'_lr_'+ str(learning_rate)+'_b_s_'+ str(batch_size ) +'_accumulation_steps_'+ str(accumulation_steps) +'_input_type_kp_arg_topic' \n",
        "  save_predictions_name  = model_name+ '__TRAIN_PREDS_'+ 'SEED_'+ str(SEED) + '_dense_layer' +'_epoc_'+ str(epochs)+'_lr_'+ str(learning_rate)+'_b_s_'+ str(batch_size ) +'_accumulation_steps_'+ str(accumulation_steps) +'_input_type_kp_arg_topic'\n",
        "\n",
        "  training_dataloader = DataLoader(train_dataset, batch_size )\n",
        "  total_steps = len(training_dataloader) * epochs\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "  \n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "                                  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "    \n",
        "  model.zero_grad()\n",
        "\n",
        "  for epoch_i in tqdm(range(epochs)):\n",
        "    y_preds = []\n",
        "    y_val = []\n",
        "    list_of_batch_losses = []\n",
        "    epoch_iterator = tqdm(training_dataloader, desc=\"Iteration\")\n",
        "    model.train()\n",
        "    \n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      if model_name in model_with_no_token_types:\n",
        "        b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_features)\n",
        "      else:\n",
        "        b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_token_type,b_features)\n",
        "            \n",
        "      b_labels = torch.reshape(b_labels, (b_labels.shape[0], 1))\n",
        "      loss = criterion(outputs, b_labels.float())\n",
        "             \n",
        "      list_of_batch_losses.append(loss.detach().cpu().numpy())\n",
        "      run[\"train/batch_loss\"].log(np.mean(loss.detach().cpu().numpy()))\n",
        "      \n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "      ypred = outputs.detach().cpu().numpy()\n",
        "      b_labels = batch[-1].cpu().detach().numpy()\n",
        "      ypred = np.hstack(ypred)\n",
        "      y_preds.append(ypred)\n",
        "\n",
        "      if (step+1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "    epoch_loss = np.mean(list_of_batch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "    run[\"train/epoch_loss\"].log(epoch_loss)\n",
        "    \n",
        "    args = df['arg_id']\n",
        "    kps = df['key_point_id']\n",
        "    true_labels = df['label']\n",
        "    topics = df['topic']\n",
        "    stances = df['stance']\n",
        "    all_preds = []\n",
        "    \n",
        "    for i in tqdm(range(len(y_preds))):\n",
        "      for p in y_preds[i]:\n",
        "        all_preds.append(p)\n",
        "\n",
        "    print('Train evaluation....')\n",
        "    \n",
        "    pred_file = pd.DataFrame({\"arg_id\" : args, \"key_point_id\": kps, \"score\": all_preds})\n",
        "    args = {}\n",
        "    kps = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg] = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg][kp] = score\n",
        "\n",
        "    with open(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', 'w') as fp:\n",
        "        fp.write(json.dumps(args))\n",
        "        fp.close()\n",
        "    \n",
        "    arg_df, kp_df, labels_df = load_kpm_data(path_dataset, subset=\"train\")\n",
        "    merged_df = get_predictions(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', labels_df, arg_df, kp_df)\n",
        "    \n",
        "    evaluate_predictions(merged_df,name = 'train')\n",
        "    \n",
        "    _,_, val_epoch_loss = evaluate_model(val_dataset,df_val, model,  model_name, mode = 'val')\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    \n",
        "\n",
        "  torch.save(model, save_model_folder +save_model+'.pt')\n",
        "  run[\"model\"].upload(save_model_folder +save_model+'.pt')\n",
        "\n",
        "  print(\"Model is saved as : \",save_model)\n",
        "  print(\"Use this to load the model\")\n",
        "    \n",
        "  return save_model,train_losses, val_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "282z5CLGUSnw",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.138385Z",
          "iopub.execute_input": "2021-09-18T09:23:55.138732Z",
          "iopub.status.idle": "2021-09-18T09:23:55.153402Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.138699Z",
          "shell.execute_reply": "2021-09-18T09:23:55.152385Z"
        },
        "trusted": true
      },
      "source": [
        "def predict_model(test_dataset,df, save_model,model_name):\n",
        "  preds = []\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "  save_predictions_name = \"TEST_PREDS_\"+ '_SEED_'+ str(SEED) + save_model\n",
        "\n",
        "  model=torch.load(save_model_folder + save_model +'.pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\")\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      model.eval()\n",
        "\n",
        "      if model_name in model_with_no_token_types:\n",
        "        b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
        "        ypred = model(b_input_ids, b_input_mask,b_features)\n",
        "      else:\n",
        "        b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device),batch[4].to(device)\n",
        "        ypred = model(b_input_ids, b_input_mask,b_token_type,b_features)\n",
        "\n",
        "      ypred = ypred.cpu().numpy()\n",
        "      ypred = np.hstack(ypred)\n",
        "\n",
        "      preds.append(ypred)\n",
        "\n",
        "  args = df['arg_id']\n",
        "  kps = df['key_point_id']\n",
        "  all_preds = []\n",
        "\n",
        "  for i in tqdm(range(len(preds))):\n",
        "    for p in preds[i]:\n",
        "      all_preds.append(p)\n",
        "\n",
        "  pred_file = pd.DataFrame({\"arg_id\" : args, \"key_point_id\": kps, \"score\": all_preds})\n",
        "\n",
        "  args = {}\n",
        "  kps = {}\n",
        "\n",
        "  for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "    args[arg] = {}\n",
        "\n",
        "  for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "    args[arg][kp] = score\n",
        "\n",
        "  with open(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', 'w') as fp:\n",
        "    fp.write(json.dumps(args))\n",
        "    fp.close()\n",
        "\n",
        "  print(\"The predictions are stored in the file : \"+ path_predictions_folder  + save_predictions_name + '_' + 'predictions.p.')\n",
        "  \n",
        "  return path_predictions_folder + save_predictions_name + '_' + 'predictions.p.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.154916Z",
          "iopub.execute_input": "2021-09-18T09:23:55.155333Z",
          "iopub.status.idle": "2021-09-18T09:23:55.171110Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.155260Z",
          "shell.execute_reply": "2021-09-18T09:23:55.170311Z"
        },
        "trusted": true,
        "id": "0Cl-Iv-0SVrk"
      },
      "source": [
        "def train_additional_dataset(train_dataset, model, filepath, model_name, batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = EPOCHS,accumulation_steps = 1):\n",
        "  losses = []\n",
        "  save_model = model_name+ '_dense_layer' +'_epoc_'+ str(epochs)+'_lr_'+ str(learning_rate)+'_b_s_'+ str(batch_size ) + '_input_type_one'\n",
        "\n",
        "  training_dataloader = DataLoader(train_dataset, batch_size )\n",
        "  total_steps = len(training_dataloader) * epochs\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "  #no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
        "  \n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "                                  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "    \n",
        "  model.zero_grad()\n",
        "  for epoch_i in tqdm(range(epochs)):\n",
        "\n",
        "    epoch_iterator = tqdm(training_dataloader, desc=\"Iteration\")\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      \n",
        "      if model_name in model_with_no_token_types:\n",
        "        b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_features)\n",
        "      \n",
        "      else:\n",
        "        b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_features)\n",
        "          \n",
        "   \n",
        "      b_labels = torch.reshape(b_labels, (b_labels.shape[0], 1))\n",
        "\n",
        "      loss = criterion(outputs, b_labels.float())\n",
        "      loss = loss / accumulation_steps  \n",
        "      losses.append(loss)\n",
        "      \n",
        "      #optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      if (step+1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        " \n",
        "  #torch.save(model, save_model_folder +save_model+'.pt')\n",
        "\n",
        "  #print(\"Model is saved as : \",save_model)\n",
        "  print(\"Model training on additional dataset is finished, but model not stored, since required ahead\")\n",
        "\n",
        "  return save_model,losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TysqDwtzUSgL",
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.172298Z",
          "iopub.execute_input": "2021-09-18T09:23:55.172692Z",
          "iopub.status.idle": "2021-09-18T09:23:55.180810Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.172655Z",
          "shell.execute_reply": "2021-09-18T09:23:55.180006Z"
        },
        "trusted": true
      },
      "source": [
        "def give_test_results(pred_file_path):\n",
        "  print('The strict and relaxed scores on the test set predictions are: ')\n",
        "  arg_df, kp_df, labels_df = load_kpm_data(gold_data_dir, subset=\"test\")\n",
        "  merged_df = get_predictions(pred_file_path, labels_df, arg_df, kp_df)\n",
        "  evaluate_predictions(merged_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-18T09:23:55.182081Z",
          "iopub.execute_input": "2021-09-18T09:23:55.182443Z",
          "iopub.status.idle": "2021-09-18T10:08:08.914289Z",
          "shell.execute_reply.started": "2021-09-18T09:23:55.182405Z",
          "shell.execute_reply": "2021-09-18T10:08:08.909385Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "3af06e960d8949fb8222f1045cb2084b",
            "58403adf9e814c03bd765cb13133a6bb",
            "51b6103bd5764abda9b93f8a6afb8c6d",
            "6b0fb20511d14fea850756c19c5eefa2",
            "f221fe5a50fd47e1b8573244da19b927",
            "7c02cbaafe494e609259df9de7bfcfe1",
            "3f24b012b4894418a9acf0d709fd020d"
          ]
        },
        "id": "O49CKKSsSVrl",
        "outputId": "7d832f07-c009-49e9-af37-df0fd19c6eec"
      },
      "source": [
        "save_model, train_additional_dataset_losses = train_additional_dataset(sts_train_dataset, model, save_model_folder, model_name = 'debertalarge', batch_size = 16, learning_rate = LEARNING_RATE, epochs = 6, accumulation_steps = ACCUMULATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3af06e960d8949fb8222f1045cb2084b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58403adf9e814c03bd765cb13133a6bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51b6103bd5764abda9b93f8a6afb8c6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b0fb20511d14fea850756c19c5eefa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f221fe5a50fd47e1b8573244da19b927"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c02cbaafe494e609259df9de7bfcfe1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f24b012b4894418a9acf0d709fd020d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Model training on additional dataset is finished, but model not stored, since required ahead\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfN_37KIUSdn",
        "execution": {
          "iopub.status.busy": "2021-09-18T10:08:08.916231Z",
          "iopub.execute_input": "2021-09-18T10:08:08.916865Z",
          "iopub.status.idle": "2021-09-18T11:18:00.916593Z",
          "shell.execute_reply.started": "2021-09-18T10:08:08.916822Z",
          "shell.execute_reply": "2021-09-18T11:18:00.914823Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "6a5366a09aee4647b396c39970b696c4",
            "d9661eec3dd84d85914232c4c1214073",
            "ec49e9892300428e922fbc21ccc310ba",
            "136ca2b6fd7945cd9d488b8bdff37752",
            "1c2a7c5dac674704a3ca4c743f71b0f7",
            "a6be1ab86bf340538b41b548ea618fe8",
            "181ce62034524d81b41b7931afb3207f",
            "d2525f3f44e34bb7a5a73330b9888e73",
            "7b692c51968642cb86d58fe0cd447399",
            "4d7e8c74ee124eed8724a189a4eabcb9",
            "e204d1dfa1db4cb0ba06a46fcdc0055d",
            "6e29c801a5654206b951cd4260e6bba6",
            "eeed003879fc424b8d7c05c4918c38ab"
          ]
        },
        "outputId": "8274d943-ed50-4d0b-ebc9-9b7bf1b88e74"
      },
      "source": [
        "save_model, train_losses, val_losses = train_and_evaluate(train_dataset,df_train, model, save_model_folder, model_name = 'debertalarge', batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = EPOCHS, accumulation_steps = ACCUMULATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a5366a09aee4647b396c39970b696c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/1506 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9661eec3dd84d85914232c4c1214073"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1506 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec49e9892300428e922fbc21ccc310ba"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train evaluation....\nmAP strict= 0.9560595225794998 ; mAP relaxed = 0.9560595225794998\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/217 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "136ca2b6fd7945cd9d488b8bdff37752"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/217 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c2a7c5dac674704a3ca4c743f71b0f7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Val evaluation....\nmAP strict= 0.9970851716755784 ; mAP relaxed = 0.9970851716755784\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/1506 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6be1ab86bf340538b41b548ea618fe8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1506 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "181ce62034524d81b41b7931afb3207f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train evaluation....\nmAP strict= 0.9852719847784277 ; mAP relaxed = 0.9852719847784277\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/217 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2525f3f44e34bb7a5a73330b9888e73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/217 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b692c51968642cb86d58fe0cd447399"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Val evaluation....\nmAP strict= 0.9978764502450063 ; mAP relaxed = 0.9978764502450063\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/1506 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d7e8c74ee124eed8724a189a4eabcb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1506 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e204d1dfa1db4cb0ba06a46fcdc0055d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train evaluation....\nmAP strict= 0.9935119302860836 ; mAP relaxed = 0.9935119302860836\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/217 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e29c801a5654206b951cd4260e6bba6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/217 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eeed003879fc424b8d7c05c4918c38ab"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Val evaluation....\nmAP strict= 0.9993738951171685 ; mAP relaxed = 0.9993738951171685\nModel is saved as :  debertalarge_SEED_0_dense_layer_epoc_3_lr_1e-05_b_s_16_accumulation_steps_2_input_type_kp_arg_topic\nUse this to load the model\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCgWwXDmeYFc",
        "execution": {
          "iopub.status.busy": "2021-09-18T11:18:00.918306Z",
          "iopub.execute_input": "2021-09-18T11:18:00.918856Z",
          "iopub.status.idle": "2021-09-18T11:19:10.046584Z",
          "shell.execute_reply.started": "2021-09-18T11:18:00.918816Z",
          "shell.execute_reply": "2021-09-18T11:19:10.045578Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "203b3c07f123432c8eee21dd0926beb5",
            "cea84d54d13b47ae87fe28362c336edb"
          ]
        },
        "outputId": "d21455bc-32ea-42bc-bf9d-84d372f1e7c2"
      },
      "source": [
        "test_preds_path = predict_model (test_dataset,df_test, save_model,model_name = 'debertalarge')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/246 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "203b3c07f123432c8eee21dd0926beb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/246 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cea84d54d13b47ae87fe28362c336edb"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "The predictions are stored in the file : TEST_PREDS__SEED_0debertalarge_SEED_0_dense_layer_epoc_3_lr_1e-05_b_s_16_accumulation_steps_2_input_type_kp_arg_topic_predictions.p.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDgGNi8leZvD",
        "execution": {
          "iopub.status.busy": "2021-09-18T11:19:10.048086Z",
          "iopub.execute_input": "2021-09-18T11:19:10.048453Z",
          "iopub.status.idle": "2021-09-18T11:19:10.193828Z",
          "shell.execute_reply.started": "2021-09-18T11:19:10.048416Z",
          "shell.execute_reply": "2021-09-18T11:19:10.192865Z"
        },
        "trusted": true,
        "outputId": "988907b8-76df-4c3e-aca6-7b54e2fee3df"
      },
      "source": [
        "give_test_results(test_preds_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The strict and relaxed scores on the test set predictions are: \nmAP strict= 0.885780390528304 ; mAP relaxed = 0.967037200671706\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N06gAbbYSVrl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}