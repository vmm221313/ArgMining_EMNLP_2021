{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Transformer on Noun or Dependency Feature on Additional and Main Dataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqC104C0UDKI",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:00.784334Z",
          "iopub.execute_input": "2021-09-16T16:38:00.784670Z",
          "iopub.status.idle": "2021-09-16T16:38:01.479919Z",
          "shell.execute_reply.started": "2021-09-16T16:38:00.784632Z",
          "shell.execute_reply": "2021-09-16T16:38:01.478986Z"
        },
        "trusted": true,
        "outputId": "bfcc7182-77f3-4f2b-d06c-e35588343dbb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Thu Sep 16 16:38:01 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2HsFgn9UTmf",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:01.481515Z",
          "iopub.execute_input": "2021-09-16T16:38:01.481864Z",
          "iopub.status.idle": "2021-09-16T16:38:12.961600Z",
          "shell.execute_reply.started": "2021-09-16T16:38:01.481826Z",
          "shell.execute_reply": "2021-09-16T16:38:12.960087Z"
        },
        "trusted": true
      },
      "source": [
        "!pip install pandarallel -q\n",
        "!pip install neptune-client -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOY5ii5TUXWy"
      },
      "source": [
        "# Importing the Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10O06vRjUTkK",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:12.964258Z",
          "iopub.execute_input": "2021-09-16T16:38:12.964667Z",
          "iopub.status.idle": "2021-09-16T16:38:20.398306Z",
          "shell.execute_reply.started": "2021-09-16T16:38:12.964623Z",
          "shell.execute_reply": "2021-09-16T16:38:20.397323Z"
        },
        "trusted": true,
        "outputId": "06f4cb67-50e7-49ab-d39a-8cb77947814d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import random as rn\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,average_precision_score, precision_score,precision_recall_curve\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm import trange\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pickle\n",
        "import nltk\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import neptune.new as neptune\n",
        "\n",
        "from torch.utils.data import (DataLoader, RandomSampler, WeightedRandomSampler, SequentialSampler, TensorDataset)\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "# Initialization\n",
        "pandarallel.initialize(progress_bar = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "INFO: Pandarallel will run on 2 workers.\nINFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUnEDSX6UTh1",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:20.400431Z",
          "iopub.execute_input": "2021-09-16T16:38:20.400965Z",
          "iopub.status.idle": "2021-09-16T16:38:22.302885Z",
          "shell.execute_reply.started": "2021-09-16T16:38:20.400924Z",
          "shell.execute_reply": "2021-09-16T16:38:22.301989Z"
        },
        "trusted": true,
        "outputId": "2e8fc176-7444-48f0-8a4d-c083265da21b"
      },
      "source": [
        "run = neptune.init(project='manav0211/emnlp',\n",
        "                   tags = 'alberta large seed 40 on dependency features',\n",
        "                   api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkMzUxMGNhMS00N2E5LTQ2YmUtYWI1Yi03ZDFhZTEzODg1NmEifQ==') # your credentials"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "https://app.neptune.ai/manav0211/emnlp/e/EM-441\nRemember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qzlV5IpUTfy",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:22.304472Z",
          "iopub.execute_input": "2021-09-16T16:38:22.304735Z",
          "iopub.status.idle": "2021-09-16T16:38:22.313453Z",
          "shell.execute_reply.started": "2021-09-16T16:38:22.304707Z",
          "shell.execute_reply": "2021-09-16T16:38:22.312453Z"
        },
        "trusted": true
      },
      "source": [
        "SEED = 0\n",
        "rn.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_-fQB9DUTdC",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:22.316981Z",
          "iopub.execute_input": "2021-09-16T16:38:22.317253Z",
          "iopub.status.idle": "2021-09-16T16:38:22.323036Z",
          "shell.execute_reply.started": "2021-09-16T16:38:22.317229Z",
          "shell.execute_reply": "2021-09-16T16:38:22.322152Z"
        },
        "trusted": true
      },
      "source": [
        "path_dataset = '../input/emnlp-full-dataset/'\n",
        "\n",
        "# use the below two lines for storing model on noun features and comment the next two lined\n",
        "#path_predictions_folder = '/content/drive/MyDrive/EMNLP_folder_4/noun_features_model_predictions_combined_dataset/'\n",
        "#save_model_folder = '/content/drive/MyDrive/EMNLP_folder_4/noun_features_model_files_combined_dataset/'\n",
        "\n",
        "# use the below two lines for storing model on dependency features and comment the previous two lines\n",
        "\n",
        "path_predictions_folder = ''\n",
        "save_model_folder = '' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqlgnES8UTam",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:22.324700Z",
          "iopub.execute_input": "2021-09-16T16:38:22.324949Z",
          "iopub.status.idle": "2021-09-16T16:38:22.339694Z",
          "shell.execute_reply.started": "2021-09-16T16:38:22.324917Z",
          "shell.execute_reply": "2021-09-16T16:38:22.336076Z"
        },
        "trusted": true,
        "outputId": "93915867-37a5-477b-db15-ecc3f4487576"
      },
      "source": [
        "max_len_arg = 55\n",
        "max_len_kp = 32\n",
        "max_len_topic = 12\n",
        "max_len_sent_1_sts = 58\n",
        "max_len_sent_2_sts = 65\n",
        "median_len_sent_1_sts = 9\n",
        "median_len_sent_2_sts = 9\n",
        "max_noun_encoded_feature_len = 66\n",
        "max_dependency_encoded_feature_len = 66\n",
        "max_len_input = 128\n",
        "model_with_no_token_types =['roberta', 'bart' ,'distilbert','deberta', 'xlmroberta', 'xlnet','xlnetlarge', 'robertalarge', 'bartlarge','debertalarge','xlmrobertalarge','albertlarge']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\nException in thread Thread-4:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 522, in _execute_operations\n    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 200, in response\n    swagger_result = self._get_swagger_result(incoming_response)\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 124, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 303, in _get_swagger_result\n    self.request_config.response_callbacks,\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 353, in unmarshal_response\n    raise_on_expected(incoming_response)\n  File \"/opt/conda/lib/python3.7/site-packages/bravado/http_future.py\", line 422, in raise_on_expected\n    swagger_result=http_response.swagger_result)\nbravado.exception.HTTPUnprocessableEntity: 422 \n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/threading/daemon.py\", line 54, in run\n    self.work()\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 177, in work\n    self.process_batch(batch, version)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/threading/daemon.py\", line 78, in wrapper\n    result = func(self_, *args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 187, in process_batch\n    result = self._processor._backend.execute_operations(self._processor._run_id, batch)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 412, in execute_operations\n    errors.extend(self._execute_operations(run_id, other_operations))\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/utils.py\", line 62, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 527, in _execute_operations\n    raise NeptuneStorageLimitException()\nneptune.new.exceptions.NeptuneStorageLimitException: \n\u001b[95m\n----NeptuneStorageLimitException---------------------------------------------------------------------------------------\n\u001b[0m\nYou exceeded storage limit for workspace. It's not possible to upload new data, but you can still fetch and delete data.\nIf you are using asynchronous (default) connection mode Neptune automatically switched to an offline mode\nand your data is being stored safely on the disk. You can upload it later using Neptune Command Line Interface:\n    \u001b[95mneptune sync -p project_name\u001b[0m\nWhat should I do?\n    - Go to your projects and remove runs or model metadata you don't need\n    - ... or update your subscription plan here: https://app.neptune.ai/-/subscription\nYou may also want to check the following docs pages:\n    - https://docs.neptune.ai/advanced-user-guides/connection-modes\n\u001b[92mNeed help?\u001b[0m-> https://docs.neptune.ai/getting-started/getting-help\n\nWarning: string series 'monitoring/stderr' value was longer than 1000 characters and was truncated. This warning is printed only once per series.\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGqp-fEapGS"
      },
      "source": [
        "### Function to make TensorDataset of the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPHZFv_RUTYL",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:31.676471Z",
          "iopub.execute_input": "2021-09-16T16:38:31.676804Z",
          "iopub.status.idle": "2021-09-16T16:38:31.690622Z",
          "shell.execute_reply.started": "2021-09-16T16:38:31.676774Z",
          "shell.execute_reply": "2021-09-16T16:38:31.689866Z"
        },
        "trusted": true
      },
      "source": [
        "def make_dataset(tokenizer, args,kps,topics,features, labels, max_len_input, model_with_no_token_types = model_with_no_token_types, model_name='roberta'):\n",
        "    \n",
        "    all_input_ids = []\n",
        "    all_token_type_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_labels = [] \n",
        "    all_features=[]\n",
        "    \n",
        "    for arg,kp,topic,feature,label in zip(args,kps,topics,features,labels) :\n",
        "\n",
        "        arg = re.sub('[^a-zA-Z]', ' ', arg)\n",
        "        kp = re.sub('[^a-zA-Z]', ' ', kp)\n",
        "        topic = re.sub('[^a-zA-Z]', ' ', topic)\n",
        "\n",
        "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        arg = url.sub(r'',arg)\n",
        "        kp = url.sub(r'',kp)\n",
        "        topic = url.sub(r'',topic)\n",
        "        \n",
        "        html=re.compile(r'<.*?>')\n",
        "        arg = html.sub(r'',arg)\n",
        "        kp = html.sub(r'',kp)\n",
        "        topic = html.sub(r'',topic)\n",
        "\n",
        "\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        \n",
        "        arg = emoji_pattern.sub(r'',arg)\n",
        "        kp = emoji_pattern.sub(r'',kp)\n",
        "        topic = emoji_pattern.sub(r'',topic)\n",
        "\n",
        "        if model_name in model_with_no_token_types:\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg+topic,max_length = max_len_input, padding='max_length')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          #all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_labels.append(label)\n",
        "          all_features.append(feature)\n",
        "\n",
        "        else :\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg+topic,max_length = max_len_input, padding='max_length')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_labels.append(label)\n",
        "          all_features.append(feature)\n",
        "          \n",
        "    if model_name in model_with_no_token_types:\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels)\n",
        "      \n",
        "      dataset = TensorDataset(all_input_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    else :\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_token_type_ids = torch.tensor(all_token_type_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels) \n",
        "\n",
        "      dataset = TensorDataset(all_input_ids,all_token_type_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:39.140493Z",
          "iopub.execute_input": "2021-09-16T16:38:39.140831Z",
          "iopub.status.idle": "2021-09-16T16:38:39.154170Z",
          "shell.execute_reply.started": "2021-09-16T16:38:39.140800Z",
          "shell.execute_reply": "2021-09-16T16:38:39.153157Z"
        },
        "trusted": true,
        "id": "zpkmNy-qRjXx"
      },
      "source": [
        "def make_dataset_additional(tokenizer, sents_1,sents_2,features, labels, max_len_input, model_with_no_token_types = model_with_no_token_types, model_name='roberta'):\n",
        "    \n",
        "    all_input_ids = []\n",
        "    all_token_type_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_features=[]\n",
        "    all_labels = [] \n",
        "    \n",
        "    for arg,kp,feature,label in zip(sents_1,sents_2,features, labels) :\n",
        "\n",
        "        arg = re.sub('[^a-zA-Z]', ' ', arg)\n",
        "        kp = re.sub('[^a-zA-Z]', ' ', kp)\n",
        "        #topic = re.sub('[^a-zA-Z]', ' ', topic)\n",
        "\n",
        "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "        arg = url.sub(r'',arg)\n",
        "        kp = url.sub(r'',kp)\n",
        "        #topic = url.sub(r'',topic)\n",
        "        \n",
        "        html=re.compile(r'<.*?>')\n",
        "        arg = html.sub(r'',arg)\n",
        "        kp = html.sub(r'',kp)\n",
        "        #topic = html.sub(r'',topic)\n",
        "\n",
        "\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        \n",
        "        arg = emoji_pattern.sub(r'',arg)\n",
        "        kp = emoji_pattern.sub(r'',kp)\n",
        "        #topic = emoji_pattern.sub(r'',topic)\n",
        "\n",
        "        if model_name in model_with_no_token_types:\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg,max_length = max_len_input, padding='max_length',truncation='longest_first')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          #all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_features.append(feature[:66])\n",
        "          all_labels.append(label)\n",
        "\n",
        "        else :\n",
        "\n",
        "          encoded_input = tokenizer(kp+arg,max_length = max_len_input, padding='max_length',truncation='longest_first')\n",
        "          all_input_ids.append(encoded_input['input_ids'])\n",
        "          all_attention_masks.append(encoded_input['attention_mask'])\n",
        "          all_token_type_ids.append(encoded_input['token_type_ids'])\n",
        "          all_features.append(feature[:66])\n",
        "          all_labels.append(label)\n",
        "          \n",
        "    if model_name in model_with_no_token_types:\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels)\n",
        "      \n",
        "      dataset = TensorDataset(all_input_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    else :\n",
        "      all_input_ids = torch.tensor(all_input_ids).squeeze()\n",
        "      all_token_type_ids = torch.tensor(all_token_type_ids).squeeze()\n",
        "      all_attention_masks = torch.tensor(all_attention_masks).squeeze()\n",
        "      all_features = torch.tensor(all_features).squeeze()\n",
        "      all_labels = torch.tensor(all_labels) \n",
        "\n",
        "      dataset = TensorDataset(all_input_ids,all_token_type_ids, all_attention_masks,all_features, all_labels)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9Frxkba6_2"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dfFMEJQUTV4",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:51.530189Z",
          "iopub.execute_input": "2021-09-16T16:38:51.530546Z",
          "iopub.status.idle": "2021-09-16T16:38:55.790555Z",
          "shell.execute_reply.started": "2021-09-16T16:38:51.530516Z",
          "shell.execute_reply": "2021-09-16T16:38:55.789644Z"
        },
        "trusted": true,
        "outputId": "4f96ede7-41d9-41ed-e2e0-00c66306b9f6"
      },
      "source": [
        "df_sts = pd.read_csv(path_dataset+'sts_dataset.csv')\n",
        "df_arg30 = pd.read_csv(path_dataset+'30k_dataset.csv')\n",
        "df_train = pd.read_csv(path_dataset+'train_tfidf.csv')\n",
        "df_val = pd.read_csv(path_dataset+'val_tfidf.csv')\n",
        "df_test  = pd.read_csv(path_dataset+'final_test.csv')\n",
        "\n",
        "print(df_train.shape,df_val.shape,df_test.shape,df_sts.shape,df_arg30.shape)\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(20635, 31) (3458, 31) (3923, 18) (8020, 16) (30497, 8)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n0           0             0               0                 0   \n1           1             1               1                 1   \n2           2             2               2                 2   \n3           3             3               3                 3   \n4           4             4               4                 4   \n\n   Unnamed: 0.1.1.1.1  Unnamed: 0.1.1.1.1.1     arg_id key_point_id  label  \\\n0                   0                     0    arg_0_0       kp_0_0      0   \n1                   1                     1  arg_0_121       kp_0_4      0   \n2                   2                     2  arg_0_121       kp_0_5      0   \n3                   3                     3  arg_0_121       kp_0_6      1   \n4                   4                     4  arg_0_121       kp_0_7      0   \n\n                                                 arg  ...  \\\n0  `people reach their limit when it comes to the...  ...   \n1  a cure or treatment may be discovered shortly ...  ...   \n2  a cure or treatment may be discovered shortly ...  ...   \n3  a cure or treatment may be discovered shortly ...  ...   \n4  a cure or treatment may be discovered shortly ...  ...   \n\n                         encoded_dependency_features  \\\n0  [1, 2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 4, 11, 12, ...   \n1  [1, 2, 3, 2, 9, 22, 4, 9, 8, 15, 12, 1, 23, 7,...   \n2  [1, 2, 3, 17, 15, 24, 23, 4, 16, 11, 9, 20, 11...   \n3  [1, 19, 9, 26, 20, 3, 27, 1, 28, 2, 9, 13, 14,...   \n4  [1, 2, 3, 6, 12, 25, 23, 19, 16, 11, 9, 20, 22...   \n\n                               encoded_noun_features  \\\n0  [1, 2, 2, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, ...   \n1  [1, 2, 2, 2, 2, 2, 2, 2, 7, 2, 1, 1, 1, 3, 1, ...   \n2  [1, 4, 4, 4, 5, 3, 1, 2, 1, 1, 2, 2, 4, 3, 2, ...   \n3  [1, 2, 2, 2, 2, 2, 4, 1, 4, 4, 4, 4, 2, 1, 4, ...   \n4  [1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 3, 2, ...   \n\n   encoded_noun_features_len encoded_dependency_features_len  \\\n0                         60                              60   \n1                         35                              35   \n2                         29                              29   \n3                         36                              36   \n4                         29                              29   \n\n                                     tf_idf_features  tf_idf_len  \\\n0  [0.5073060076370642, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31489661...         298   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         298   \n\n                              tf_idf_bigram_features  tf_idf_bigram_len  \\\n0  [0.419428095234635, 0.0, 0.0, 0.0, 0.0, 0.0, 0...                329   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                329   \n\n                             tf_idf_trigram_features tf_idf_trigram_len  \n0  [0.38897968136796685, 0.0, 0.0, 0.0, 0.0, 0.0,...                684  \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                684  \n\n[5 rows x 31 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0.1.1</th>\n      <th>Unnamed: 0.1.1.1</th>\n      <th>Unnamed: 0.1.1.1.1</th>\n      <th>Unnamed: 0.1.1.1.1.1</th>\n      <th>arg_id</th>\n      <th>key_point_id</th>\n      <th>label</th>\n      <th>arg</th>\n      <th>...</th>\n      <th>encoded_dependency_features</th>\n      <th>encoded_noun_features</th>\n      <th>encoded_noun_features_len</th>\n      <th>encoded_dependency_features_len</th>\n      <th>tf_idf_features</th>\n      <th>tf_idf_len</th>\n      <th>tf_idf_bigram_features</th>\n      <th>tf_idf_bigram_len</th>\n      <th>tf_idf_trigram_features</th>\n      <th>tf_idf_trigram_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>arg_0_0</td>\n      <td>kp_0_0</td>\n      <td>0</td>\n      <td>`people reach their limit when it comes to the...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 4, 11, 12, ...</td>\n      <td>[1, 2, 2, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 1, 2, ...</td>\n      <td>60</td>\n      <td>60</td>\n      <td>[0.5073060076370642, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.419428095234635, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n      <td>329</td>\n      <td>[0.38897968136796685, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>arg_0_121</td>\n      <td>kp_0_4</td>\n      <td>0</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 2, 9, 22, 4, 9, 8, 15, 12, 1, 23, 7,...</td>\n      <td>[1, 2, 2, 2, 2, 2, 2, 2, 7, 2, 1, 1, 1, 3, 1, ...</td>\n      <td>35</td>\n      <td>35</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>arg_0_121</td>\n      <td>kp_0_5</td>\n      <td>0</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 17, 15, 24, 23, 4, 16, 11, 9, 20, 11...</td>\n      <td>[1, 4, 4, 4, 5, 3, 1, 2, 1, 1, 2, 2, 4, 3, 2, ...</td>\n      <td>29</td>\n      <td>29</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31489661...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>arg_0_121</td>\n      <td>kp_0_6</td>\n      <td>1</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 19, 9, 26, 20, 3, 27, 1, 28, 2, 9, 13, 14,...</td>\n      <td>[1, 2, 2, 2, 2, 2, 4, 1, 4, 4, 4, 4, 2, 1, 4, ...</td>\n      <td>36</td>\n      <td>36</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>arg_0_121</td>\n      <td>kp_0_7</td>\n      <td>0</td>\n      <td>a cure or treatment may be discovered shortly ...</td>\n      <td>...</td>\n      <td>[1, 2, 3, 6, 12, 25, 23, 19, 16, 11, 9, 20, 22...</td>\n      <td>[1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 3, 2, ...</td>\n      <td>29</td>\n      <td>29</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>298</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>329</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>684</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-16T16:44:26.380230Z",
          "iopub.execute_input": "2021-09-16T16:44:26.380560Z",
          "iopub.status.idle": "2021-09-16T16:44:26.390827Z",
          "shell.execute_reply.started": "2021-09-16T16:44:26.380531Z",
          "shell.execute_reply": "2021-09-16T16:44:26.389908Z"
        },
        "trusted": true,
        "id": "q8mlbbFkRjXy",
        "outputId": "319c6e0f-3219-4675-bbe9-e1c6108aa12b"
      },
      "source": [
        "max(df_train['encoded_noun_features_len']),max(df_val['encoded_noun_features_len']),max(df_test['encoded_noun_features_len'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(66, 60, 60)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-16T16:44:54.807075Z",
          "iopub.execute_input": "2021-09-16T16:44:54.807410Z",
          "iopub.status.idle": "2021-09-16T16:44:54.817103Z",
          "shell.execute_reply.started": "2021-09-16T16:44:54.807379Z",
          "shell.execute_reply": "2021-09-16T16:44:54.816254Z"
        },
        "trusted": true,
        "id": "ceHTP63hRjXz",
        "outputId": "c588bd98-8657-4d9c-8ccd-18912c3c9f1f"
      },
      "source": [
        "max(df_train['encoded_dependency_features_len']),max(df_val['encoded_dependency_features_len']),max(df_test['encoded_dependency_features_len'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(66, 60, 60)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekvTJTEeUTTZ",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:55.792929Z",
          "iopub.execute_input": "2021-09-16T16:38:55.793280Z",
          "iopub.status.idle": "2021-09-16T16:38:55.814881Z",
          "shell.execute_reply.started": "2021-09-16T16:38:55.793252Z",
          "shell.execute_reply": "2021-09-16T16:38:55.814123Z"
        },
        "trusted": true,
        "outputId": "4f3286da-986f-4c7c-ad29-92c3165671f4"
      },
      "source": [
        "df_sts.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n0           0             0               0   \n1           1             1               1   \n2           2             2               2   \n3           3             3               3   \n4           4             4               4   \n\n                                          sent_1  \\\n0                         A plane is taking off.   \n1                A man is playing a large flute.   \n2  A man is spreading shreded cheese on a pizza.   \n3                   Three men are playing chess.   \n4                    A man is playing the cello.   \n\n                                              sent_2  label  label_normalized  \\\n0                        An air plane is taking off.   5.00              1.00   \n1                          A man is playing a flute.   3.80              0.76   \n2  A man is spreading shredded cheese on an uncoo...   3.80              0.76   \n3                         Two men are playing chess.   2.60              0.52   \n4                 A man seated is playing the cello.   4.25              0.85   \n\n   sent_1_token_lengths  sent_2_token_lengths  \\\n0                     6                     7   \n1                     9                     8   \n2                    11                    12   \n3                     6                     6   \n4                     8                     9   \n\n                                      input_sentence  \\\n0  A plane is taking off.An air plane is taking off.   \n1  A man is playing a large flute.A man is playin...   \n2  A man is spreading shreded cheese on a pizza.A...   \n3  Three men are playing chess.Two men are playin...   \n4  A man is playing the cello.A man seated is pla...   \n\n                                 dependency_features  \\\n0  ['det', 'nsubj', 'aux', 'ROOT', 'prt', 'punct'...   \n1  ['det', 'nsubj', 'aux', 'ROOT', 'det', 'amod',...   \n2  ['det', 'nsubj', 'aux', 'ROOT', 'amod', 'dobj'...   \n3  ['nummod', 'nsubj', 'aux', 'ROOT', 'dobj', 'pu...   \n4  ['det', 'nsubj', 'aux', 'ROOT', 'det', 'dobj',...   \n\n                                       noun_features  \\\n0  ['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...   \n1  ['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'NOUN...   \n2  ['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...   \n3  ['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...   \n4  ['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...   \n\n                         encoded_dependency_features  \\\n0  [1, 2, 3, 4, 5, 6, 1, 7, 2, 3, 4, 5, 6, 0, 0, ...   \n1  [1, 2, 3, 4, 1, 8, 9, 6, 1, 2, 3, 4, 1, 9, 6, ...   \n2  [1, 2, 3, 4, 8, 9, 10, 1, 11, 6, 1, 2, 3, 4, 8...   \n3  [12, 2, 3, 4, 9, 6, 12, 2, 3, 4, 9, 6, 0, 0, 0...   \n4  [1, 2, 3, 4, 1, 9, 6, 1, 2, 13, 3, 4, 1, 9, 6,...   \n\n                               encoded_noun_features  \\\n0  [1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, ...   \n1  [1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, ...   \n2  [1, 2, 2, 2, 1, 2, 2, 1, 3, 2, 1, 2, 2, 2, 1, ...   \n3  [1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, ...   \n4  [1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, ...   \n\n   encoded_noun_features_len  encoded_dependency_features_len  \n0                         84                               84  \n1                         84                               84  \n2                         84                               84  \n3                         84                               84  \n4                         84                               84  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 0.1</th>\n      <th>Unnamed: 0.1.1</th>\n      <th>sent_1</th>\n      <th>sent_2</th>\n      <th>label</th>\n      <th>label_normalized</th>\n      <th>sent_1_token_lengths</th>\n      <th>sent_2_token_lengths</th>\n      <th>input_sentence</th>\n      <th>dependency_features</th>\n      <th>noun_features</th>\n      <th>encoded_dependency_features</th>\n      <th>encoded_noun_features</th>\n      <th>encoded_noun_features_len</th>\n      <th>encoded_dependency_features_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>A plane is taking off.</td>\n      <td>An air plane is taking off.</td>\n      <td>5.00</td>\n      <td>1.00</td>\n      <td>6</td>\n      <td>7</td>\n      <td>A plane is taking off.An air plane is taking off.</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'prt', 'punct'...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...</td>\n      <td>[1, 2, 3, 4, 5, 6, 1, 7, 2, 3, 4, 5, 6, 0, 0, ...</td>\n      <td>[1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, ...</td>\n      <td>84</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>A man is playing a large flute.</td>\n      <td>A man is playing a flute.</td>\n      <td>3.80</td>\n      <td>0.76</td>\n      <td>9</td>\n      <td>8</td>\n      <td>A man is playing a large flute.A man is playin...</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'det', 'amod',...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'NOUN...</td>\n      <td>[1, 2, 3, 4, 1, 8, 9, 6, 1, 2, 3, 4, 1, 9, 6, ...</td>\n      <td>[1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, ...</td>\n      <td>84</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>A man is spreading shreded cheese on a pizza.</td>\n      <td>A man is spreading shredded cheese on an uncoo...</td>\n      <td>3.80</td>\n      <td>0.76</td>\n      <td>11</td>\n      <td>12</td>\n      <td>A man is spreading shreded cheese on a pizza.A...</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'amod', 'dobj'...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...</td>\n      <td>[1, 2, 3, 4, 8, 9, 10, 1, 11, 6, 1, 2, 3, 4, 8...</td>\n      <td>[1, 2, 2, 2, 1, 2, 2, 1, 3, 2, 1, 2, 2, 2, 1, ...</td>\n      <td>84</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>Three men are playing chess.</td>\n      <td>Two men are playing chess.</td>\n      <td>2.60</td>\n      <td>0.52</td>\n      <td>6</td>\n      <td>6</td>\n      <td>Three men are playing chess.Two men are playin...</td>\n      <td>['nummod', 'nsubj', 'aux', 'ROOT', 'dobj', 'pu...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'VERB', 'VERB...</td>\n      <td>[12, 2, 3, 4, 9, 6, 12, 2, 3, 4, 9, 6, 0, 0, 0...</td>\n      <td>[1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, ...</td>\n      <td>84</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>A man is playing the cello.</td>\n      <td>A man seated is playing the cello.</td>\n      <td>4.25</td>\n      <td>0.85</td>\n      <td>8</td>\n      <td>9</td>\n      <td>A man is playing the cello.A man seated is pla...</td>\n      <td>['det', 'nsubj', 'aux', 'ROOT', 'det', 'dobj',...</td>\n      <td>['NOUN', 'VERB', 'VERB', 'VERB', 'NOUN', 'VERB...</td>\n      <td>[1, 2, 3, 4, 1, 9, 6, 1, 2, 13, 3, 4, 1, 9, 6,...</td>\n      <td>[1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, ...</td>\n      <td>84</td>\n      <td>84</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBXpAIAfUTRB",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:38:55.816673Z",
          "iopub.execute_input": "2021-09-16T16:38:55.817189Z",
          "iopub.status.idle": "2021-09-16T16:38:55.830783Z",
          "shell.execute_reply.started": "2021-09-16T16:38:55.817151Z",
          "shell.execute_reply": "2021-09-16T16:38:55.829398Z"
        },
        "trusted": true,
        "outputId": "d94b6368-ed0f-4070-9a5b-08b7cedb89f3"
      },
      "source": [
        "df_arg30.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Unnamed: 0                                                arg  \\\n0           0  \"marriage\" isn't keeping up with the times.  a...   \n1           1  .a multi-party system would be too confusing a...   \n2           2  \\ero-tolerance policy in schools should not be...   \n3           3  `people reach their limit when it comes to the...   \n4           4  100% agree, should they do that, it would be a...   \n\n                                               topic    set        WA  \\\n0                         We should abandon marriage  train  0.846165   \n1               We should adopt a multi-party system  train  0.891271   \n2  We should adopt a zero-tolerance policy in sch...    dev  0.721192   \n3      Assisted suicide should be a criminal offence  train  0.730395   \n4                      We should abolish safe spaces  train  0.236686   \n\n      label  stance_WA  stance_WA_conf  \n0  0.297659          1        1.000000  \n1  0.726133         -1        1.000000  \n2  0.396953         -1        1.000000  \n3  0.225212         -1        1.000000  \n4  0.004104          1        0.805517  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>arg</th>\n      <th>topic</th>\n      <th>set</th>\n      <th>WA</th>\n      <th>label</th>\n      <th>stance_WA</th>\n      <th>stance_WA_conf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>\"marriage\" isn't keeping up with the times.  a...</td>\n      <td>We should abandon marriage</td>\n      <td>train</td>\n      <td>0.846165</td>\n      <td>0.297659</td>\n      <td>1</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>.a multi-party system would be too confusing a...</td>\n      <td>We should adopt a multi-party system</td>\n      <td>train</td>\n      <td>0.891271</td>\n      <td>0.726133</td>\n      <td>-1</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>\\ero-tolerance policy in schools should not be...</td>\n      <td>We should adopt a zero-tolerance policy in sch...</td>\n      <td>dev</td>\n      <td>0.721192</td>\n      <td>0.396953</td>\n      <td>-1</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>`people reach their limit when it comes to the...</td>\n      <td>Assisted suicide should be a criminal offence</td>\n      <td>train</td>\n      <td>0.730395</td>\n      <td>0.225212</td>\n      <td>-1</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>100% agree, should they do that, it would be a...</td>\n      <td>We should abolish safe spaces</td>\n      <td>train</td>\n      <td>0.236686</td>\n      <td>0.004104</td>\n      <td>1</td>\n      <td>0.805517</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz6pmKVqUTOi",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:39:11.555400Z",
          "iopub.execute_input": "2021-09-16T16:39:11.555783Z",
          "iopub.status.idle": "2021-09-16T16:39:11.588796Z",
          "shell.execute_reply.started": "2021-09-16T16:39:11.555752Z",
          "shell.execute_reply": "2021-09-16T16:39:11.587729Z"
        },
        "trusted": true,
        "outputId": "de415c12-ccb1-4d10-818a-4557a4fcbadf"
      },
      "source": [
        "# Here I am concatenating train and dev set for training the model for creating test dataset predictions\n",
        "df_train = pd.concat([df_train,df_val])\n",
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(24093, 31)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvAKOZ8-bMUf",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:39:13.271450Z",
          "iopub.execute_input": "2021-09-16T16:39:13.271794Z",
          "iopub.status.idle": "2021-09-16T16:39:13.276140Z",
          "shell.execute_reply.started": "2021-09-16T16:39:13.271763Z",
          "shell.execute_reply": "2021-09-16T16:39:13.275014Z"
        },
        "trusted": true
      },
      "source": [
        "def give_list(sent):\n",
        "  res = ast.literal_eval(sent)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsgumEs9bMR5",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:43:19.996749Z",
          "iopub.execute_input": "2021-09-16T16:43:19.997132Z",
          "iopub.status.idle": "2021-09-16T16:43:25.567652Z",
          "shell.execute_reply.started": "2021-09-16T16:43:19.997097Z",
          "shell.execute_reply": "2021-09-16T16:43:25.566682Z"
        },
        "trusted": true,
        "outputId": "7bee452c-ae99-4314-ac6b-723d15fe8680",
        "colab": {
          "referenced_widgets": [
            "273740fefb1244229e8176c949b967ff",
            "dd0aef63d82143d39c6408f031178433",
            "f32d5f15d02e45febaeea5edb5b7b8e7",
            "2188b4e588594fffa2c01bd1353165d4"
          ]
        }
      },
      "source": [
        "## For using noun features use the first three lines of code \n",
        "## For using dependency features use the last three lines of code and comment the other three accordingly\n",
        "\n",
        "df_train['encoded_noun_features']= df_train['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "df_val['encoded_noun_features']= df_val['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "df_test['encoded_noun_features']= df_test['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "df_sts['encoded_noun_features']= df_sts['encoded_noun_features'].parallel_apply(lambda x: give_list(x))\n",
        "\n",
        "\n",
        "#df_train['encoded_dependency_features']= df_train['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_val['encoded_dependency_features']= df_val['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))\n",
        "#df_test['encoded_dependency_features']= df_test['encoded_dependency_features'].parallel_apply(lambda x: give_list(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=12047), Label(value='0 / 12047')))â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "273740fefb1244229e8176c949b967ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1729), Label(value='0 / 1729'))), â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd0aef63d82143d39c6408f031178433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1962), Label(value='0 / 1962'))), â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f32d5f15d02e45febaeea5edb5b7b8e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4010), Label(value='0 / 4010'))), â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2188b4e588594fffa2c01bd1353165d4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhIbcJzbD6E"
      },
      "source": [
        "types_of_models= model : tokenizer model_path\n",
        " - 'bert':  'bert-base-uncased'\n",
        " - 'roberta':  'roberta-base'\n",
        " - 'bart':  \"facebook/bart-base\"\n",
        " - 'distilbert': 'distilbert-base-uncased'\n",
        " - 'deberta': 'microsoft/deberta-base'\n",
        " - 'debertalarge': 'microsoft/deberta-large'\n",
        " - 'xlnet' : 'xlnet-base-cased'\n",
        " - 'xlnetlarge' : 'xlnet-large-cased'\n",
        " - 'xlmrobertalarge' : 'xlm-roberta-large'\n",
        " - 'bartlarge' : 'facebook/bart-large'\n",
        " - 'bertlarge':  'bert-large-uncased'\n",
        " - 'robertalarge':  'roberta-large'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vh9cVerUTMS",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:43:31.066894Z",
          "iopub.execute_input": "2021-09-16T16:43:31.067290Z",
          "iopub.status.idle": "2021-09-16T16:43:31.072131Z",
          "shell.execute_reply.started": "2021-09-16T16:43:31.067248Z",
          "shell.execute_reply": "2021-09-16T16:43:31.071244Z"
        },
        "trusted": true
      },
      "source": [
        "model_name = 'bartlarge'\n",
        "model_path = 'facebook/bart-large'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcdpBLs4UTJ6",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:43:31.229869Z",
          "iopub.execute_input": "2021-09-16T16:43:31.230210Z",
          "iopub.status.idle": "2021-09-16T16:43:38.526604Z",
          "shell.execute_reply.started": "2021-09-16T16:43:31.230178Z",
          "shell.execute_reply": "2021-09-16T16:43:38.525689Z"
        },
        "trusted": true,
        "outputId": "9ac507b2-0b59-4e1b-8be0-7ed893d338b7",
        "colab": {
          "referenced_widgets": [
            "7997ba219e8c44d8b72f383e4f886efe",
            "3b3abf0eb1524c3084a55468290af307",
            "e14790db28874c87a1be1ec314fa0700",
            "9e1a3b26549c4916b74ee2568ef84588",
            "5bf2d3c633f9441b819ba2da04f107aa"
          ]
        }
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.60k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7997ba219e8c44d8b72f383e4f886efe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b3abf0eb1524c3084a55468290af307"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e14790db28874c87a1be1ec314fa0700"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e1a3b26549c4916b74ee2568ef84588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bf2d3c633f9441b819ba2da04f107aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tp5FYy4UTHX",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:45:52.217682Z",
          "iopub.execute_input": "2021-09-16T16:45:52.218015Z",
          "iopub.status.idle": "2021-09-16T16:46:04.802091Z",
          "shell.execute_reply.started": "2021-09-16T16:45:52.217983Z",
          "shell.execute_reply": "2021-09-16T16:46:04.801156Z"
        },
        "trusted": true
      },
      "source": [
        "## For using noun features use the first three lines of code \n",
        "## For using dependency features use the last three lines of code and comment the other three accordingly\n",
        "\n",
        "sts_train_dataset = make_dataset_additional(tokenizer, df_sts['sent_1'],df_sts['sent_2'],df_sts['encoded_noun_features'], df_sts['label_normalized'], max_len_input,model_with_no_token_types, model_name=model_name)\n",
        "train_dataset = make_dataset(tokenizer, df_train['arg'], df_train['key_point'], df_train['topic'],df_train['encoded_noun_features'], df_train['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "val_dataset = make_dataset(tokenizer, df_val['arg'], df_val['key_point'], df_val['topic'],df_val['encoded_noun_features'], df_val['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "test_dataset = make_dataset(tokenizer, df_test['arg'], df_test['key_point'], df_test['topic'],df_test['encoded_noun_features'], df_test['stance'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "\n",
        "\n",
        "#train_dataset = make_dataset(tokenizer, df_train['arg'], df_train['key_point'], df_train['topic'],df_train['encoded_dependency_features'], df_train['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "#val_dataset = make_dataset(tokenizer, df_val['arg'], df_val['key_point'], df_val['topic'],df_val['encoded_dependency_features'], df_val['label'], max_len_input, model_with_no_token_types, model_name=model_name)\n",
        "#test_dataset = make_dataset(tokenizer, df_test['arg'], df_test['key_point'], df_test['topic'],df_test['encoded_dependency_features'], df_test['stance'], max_len_input, model_with_no_token_types, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyrsI4_RbcWp"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLb0hjRLUTEr",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:46:04.803619Z",
          "iopub.execute_input": "2021-09-16T16:46:04.803941Z",
          "iopub.status.idle": "2021-09-16T16:46:04.814815Z",
          "shell.execute_reply.started": "2021-09-16T16:46:04.803907Z",
          "shell.execute_reply": "2021-09-16T16:46:04.813984Z"
        },
        "trusted": true
      },
      "source": [
        "# Use this Class only for Bert base and Bert large model\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        #Instantiating Pre trained model object \n",
        "        self.model_layer = AutoModel.from_pretrained(model_path)\n",
        "        \n",
        "        #Layers\n",
        "        # the first dense layer will have 834 neurons if base model is used and \n",
        "        # 1090 neurons if large model is used\n",
        "\n",
        "        self.dense_layer_1 = nn.Linear(1090, 256)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.dense_layer_2 = nn.Linear(256, 128)\n",
        "        self.dropout_2 = nn.Dropout(0.4) \n",
        "        self.cls_layer = nn.Linear(128, 1, bias = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,input_ids, attention_masks, token_type_ids, features):\n",
        "\n",
        "        pooled_output = self.model_layer(input_ids=input_ids, attention_mask=attention_masks,token_type_ids = token_type_ids).pooler_output\n",
        "\n",
        "        ## Combining the noun features and bert pooler output\n",
        "        concat = torch.cat((pooled_output,features),dim =1)\n",
        "        \n",
        "        x = self.dense_layer_1(concat)\n",
        "        x = self.dropout(x)\n",
        "        x_1 = self.dense_layer_2(x)\n",
        "        x_2 = self.dropout_2(x_1)\n",
        "        \n",
        "        logits = self.cls_layer(x_2)\n",
        "        output = self.sigmoid(logits)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZAK_TSoUTCD",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:46:04.816668Z",
          "iopub.execute_input": "2021-09-16T16:46:04.817312Z",
          "iopub.status.idle": "2021-09-16T16:46:04.827198Z",
          "shell.execute_reply.started": "2021-09-16T16:46:04.817270Z",
          "shell.execute_reply": "2021-09-16T16:46:04.826011Z"
        },
        "trusted": true
      },
      "source": [
        "# Use this Class for the rest of transformer models\n",
        "\n",
        "class NonPoolerTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NonPoolerTransformer, self).__init__()\n",
        "        \n",
        "        #Instantiating Pre trained model object \n",
        "        self.model_layer = AutoModel.from_pretrained(model_path)\n",
        "        \n",
        "        #Layers\n",
        "        # the first dense layer will have 834 neurons if base model is used and \n",
        "        # 1090 neurons if large model is used\n",
        "\n",
        "        self.dense_layer_1 = nn.Linear(1090, 256)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self.dense_layer_2 = nn.Linear(256, 128)\n",
        "        self.dropout_2 = nn.Dropout(0.4)\n",
        "        self.cls_layer = nn.Linear(128, 1, bias = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,input_ids, attention_masks,features):\n",
        "\n",
        "        hidden_state = self.model_layer(input_ids=input_ids, attention_mask=attention_masks)[0]\n",
        "        pooled_output = hidden_state[:, 0]\n",
        "\n",
        "        ## Combining the noun features and model pooler output\n",
        "        concat = torch.cat((pooled_output,features),dim =1)\n",
        "\n",
        "        x = self.dense_layer_1(concat)\n",
        "        x = self.dropout(x)\n",
        "        x_1 = self.dense_layer_2(x)\n",
        "        x_2 = self.dropout_2(x_1)\n",
        "        \n",
        "        logits = self.cls_layer(x_2)\n",
        "        output = self.sigmoid(logits)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XpYQFpOUS_s",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:46:06.977964Z",
          "iopub.execute_input": "2021-09-16T16:46:06.978355Z",
          "iopub.status.idle": "2021-09-16T16:46:56.803900Z",
          "shell.execute_reply.started": "2021-09-16T16:46:06.978321Z",
          "shell.execute_reply": "2021-09-16T16:46:56.802936Z"
        },
        "trusted": true,
        "outputId": "f411f421-9b58-41d6-916c-61f160cde447",
        "colab": {
          "referenced_widgets": [
            "54695695188a4dca9608b0f3faa93ead"
          ]
        }
      },
      "source": [
        "# comment the nonpoolertransformer line for bert model and the transformer model line for the rest of the transformer models\n",
        "\n",
        "model = NonPoolerTransformer()\n",
        "#model = Transformer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.02G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54695695188a4dca9608b0f3faa93ead"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y06EazsOUS9B",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:46:56.805542Z",
          "iopub.execute_input": "2021-09-16T16:46:56.805891Z",
          "iopub.status.idle": "2021-09-16T16:47:02.882443Z",
          "shell.execute_reply.started": "2021-09-16T16:46:56.805854Z",
          "shell.execute_reply": "2021-09-16T16:47:02.881521Z"
        },
        "trusted": true
      },
      "source": [
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF9w7JvlUS6k",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.884195Z",
          "iopub.execute_input": "2021-09-16T16:47:02.884517Z",
          "iopub.status.idle": "2021-09-16T16:47:02.888087Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.884488Z",
          "shell.execute_reply": "2021-09-16T16:47:02.887306Z"
        },
        "trusted": true
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 3\n",
        "ACCUMULATION_STEPS = 2\n",
        "DROPOUT = 0.4\n",
        "gold_data_dir = path_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Kz81sxUS4J",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.889653Z",
          "iopub.execute_input": "2021-09-16T16:47:02.890148Z",
          "iopub.status.idle": "2021-09-16T16:47:02.901860Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.890112Z",
          "shell.execute_reply": "2021-09-16T16:47:02.901026Z"
        },
        "trusted": true
      },
      "source": [
        "PARAMS = {'model_name': model_name,'model_path': model_path,'lr': LEARNING_RATE, 'epoch_nr': EPOCHS, 'batch_size': BATCH_SIZE, 'accumulation_steps': ACCUMULATION_STEPS,'dropout': DROPOUT}\n",
        "run['details'] = PARAMS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG21pTKUc4mj"
      },
      "source": [
        "### Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZqgD1lmUS14",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.903168Z",
          "iopub.execute_input": "2021-09-16T16:47:02.903577Z",
          "iopub.status.idle": "2021-09-16T16:47:02.910211Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.903542Z",
          "shell.execute_reply": "2021-09-16T16:47:02.909298Z"
        },
        "trusted": true
      },
      "source": [
        "def load_kpm_data(gold_data_dir, subset):\n",
        "    \n",
        "    arguments_file = os.path.join(gold_data_dir, f\"arguments_{subset}.csv\")\n",
        "    key_points_file = os.path.join(gold_data_dir, f\"key_points_{subset}.csv\")\n",
        "    labels_file = os.path.join(gold_data_dir, f\"labels_{subset}.csv\")\n",
        "\n",
        "    arguments_df = pd.read_csv(arguments_file)\n",
        "    key_points_df = pd.read_csv(key_points_file)\n",
        "    labels_file_df = pd.read_csv(labels_file)\n",
        "    \n",
        "    return arguments_df, key_points_df, labels_file_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GNshalEUSzY",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.911574Z",
          "iopub.execute_input": "2021-09-16T16:47:02.912147Z",
          "iopub.status.idle": "2021-09-16T16:47:02.921193Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.912110Z",
          "shell.execute_reply": "2021-09-16T16:47:02.920375Z"
        },
        "trusted": true
      },
      "source": [
        "def get_predictions(predictions_file, labels_df, arg_df, kp_df):\n",
        "    #print(\"\\nÖ¿** loading predictions:\")\n",
        "    arg_df = arg_df[[\"arg_id\", \"topic\", \"stance\"]]\n",
        "    predictions_df = load_predictions(predictions_file, kp_df[\"key_point_id\"].unique())\n",
        "\n",
        "    #make sure each arg_id has a prediction\n",
        "    predictions_df = pd.merge(arg_df, predictions_df, how=\"left\", on=\"arg_id\")\n",
        "\n",
        "    #handle arguements with no matching key point\n",
        "    predictions_df[\"key_point_id\"] = predictions_df[\"key_point_id\"].fillna(\"dummy_id\")\n",
        "    predictions_df[\"score\"] = predictions_df[\"score\"].fillna(0)\n",
        "\n",
        "    #merge each argument with the gold labels\n",
        "    merged_df = pd.merge(predictions_df, labels_df, how=\"left\", on=[\"arg_id\", \"key_point_id\"])\n",
        "\n",
        "    merged_df.loc[merged_df['key_point_id'] == \"dummy_id\", 'label'] = 0\n",
        "    merged_df[\"label_strict\"] = merged_df[\"label\"].fillna(0)\n",
        "    merged_df[\"label_relaxed\"] = merged_df[\"label\"].fillna(1)\n",
        "\n",
        "    return merged_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BfGWC5LUSw-",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.922522Z",
          "iopub.execute_input": "2021-09-16T16:47:02.922892Z",
          "iopub.status.idle": "2021-09-16T16:47:02.932719Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.922856Z",
          "shell.execute_reply": "2021-09-16T16:47:02.931720Z"
        },
        "trusted": true
      },
      "source": [
        "def load_predictions(predictions_dir, correct_kp_list):\n",
        "    arg =[]\n",
        "    kp = []\n",
        "    scores = []\n",
        "    invalid_keypoints = set()\n",
        "    with open(predictions_dir, \"r\") as f_in:\n",
        "        res = json.load(f_in)\n",
        "        for arg_id, kps in res.items():\n",
        "            valid_kps = {key: value for key, value in kps.items() if key in correct_kp_list}\n",
        "            invalid = {key: value for key, value in kps.items() if key not in correct_kp_list}\n",
        "            for invalid_kp, _ in invalid.items():\n",
        "                if invalid_kp not in invalid_keypoints:\n",
        "                    #print(f\"key point {invalid_kp} doesn't appear in the key points file and will be ignored\")\n",
        "                    invalid_keypoints.add(invalid_kp)\n",
        "            if valid_kps:\n",
        "                best_kp = max(valid_kps.items(), key=lambda x: x[1])\n",
        "                arg.append(arg_id)\n",
        "                kp.append(best_kp[0])\n",
        "                scores.append(best_kp[1])\n",
        "        #print(f\"\\tloaded predictions for {len(arg)} arguments\")\n",
        "        \n",
        "        return pd.DataFrame({\"arg_id\" : arg, \"key_point_id\": kp, \"score\": scores})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdXgzQn6USuh",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.935493Z",
          "iopub.execute_input": "2021-09-16T16:47:02.935880Z",
          "iopub.status.idle": "2021-09-16T16:47:02.945690Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.935845Z",
          "shell.execute_reply": "2021-09-16T16:47:02.944762Z"
        },
        "trusted": true
      },
      "source": [
        "def get_ap(df, label_column, top_percentile=0.5):\n",
        "    top = int(len(df)*top_percentile)\n",
        "    df = df.sort_values('score', ascending=False).head(top)\n",
        "    # after selecting top percentile candidates, we set the score for the dummy kp to 1, to prevent it from increasing the precision.\n",
        "    df.loc[df['key_point_id'] == \"dummy_id\", 'score'] = 0.99\n",
        "    return average_precision_score(y_true=df[label_column], y_score=df[\"score\"])\n",
        "\n",
        "def calc_mean_average_precision(df, label_column):\n",
        "    precisions = [get_ap(group, label_column) for _, group in df.groupby([\"topic\", \"stance\"])]\n",
        "    return np.mean(precisions)\n",
        "\n",
        "def evaluate_predictions(merged_df,name = 'train'):\n",
        "    #print(\"\\n** running evalution:\")\n",
        "    mAP_strict = calc_mean_average_precision(merged_df, \"label_strict\")\n",
        "    mAP_relaxed = calc_mean_average_precision(merged_df, \"label_relaxed\")\n",
        "    # below two lines are added for neptune results logging\n",
        "    run[f\"{name}/map\"].log(mAP_strict)\n",
        "    run[f\"{name}/map_relaxed\"].log(mAP_relaxed)\n",
        "                         \n",
        "    print(f\"mAP strict= {mAP_strict} ; mAP relaxed = {mAP_relaxed}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6MfrZRmdGFV"
      },
      "source": [
        "### Train and Predict Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo2ZBjKZUSsE",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.947221Z",
          "iopub.execute_input": "2021-09-16T16:47:02.947724Z",
          "iopub.status.idle": "2021-09-16T16:47:02.967689Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.947686Z",
          "shell.execute_reply": "2021-09-16T16:47:02.966862Z"
        },
        "trusted": true
      },
      "source": [
        "def evaluate_model(test_dataset,df, model,  model_name, mode = 'train'):\n",
        "    \n",
        "    save_predictions_name = model_name+ '__VAL_PREDS_'+ 'SEED_'+ str(SEED) + '_dense_layer' +'_epoc_'+ str(EPOCHS)+'_lr_'+ str(LEARNING_RATE)+'_b_s_'+ str(BATCH_SIZE) +'_accumulation_steps_'+ str(ACCUMULATION_STEPS) +'_input_type_kp_arg_topic_feature'\n",
        "\n",
        "    y_preds = []\n",
        "    val_losses = []\n",
        "    criterion = nn.BCELoss()\n",
        "    list_of_batch_losses = []\n",
        "    \n",
        "    if mode in ['train','val']:\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        acc_epoch = []\n",
        "        \n",
        "        epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.eval()\n",
        "            \n",
        "            if model_name in model_with_no_token_types:\n",
        "                b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device),batch[3].to(device)\n",
        "                ypred = model(b_input_ids, b_input_mask,b_features)\n",
        "            else:\n",
        "                b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device),batch[4].to(device)\n",
        "                ypred = model(b_input_ids, b_input_mask,b_token_type,b_features)\n",
        "                \n",
        "            b_labels_copy = torch.reshape(b_labels, (b_labels.shape[0], 1))\n",
        "            loss_batch = criterion(ypred, b_labels_copy.float())\n",
        "            list_of_batch_losses.append(loss_batch.detach().cpu().numpy())\n",
        "            run[\"val/batch_loss\"].log(np.mean(loss_batch.detach().cpu().numpy()))\n",
        "            \n",
        "            ypred = ypred.cpu().numpy()\n",
        "            b_labels = batch[-1].cpu().detach().numpy()\n",
        "        \n",
        "            ypred = np.hstack(ypred)\n",
        "            y_preds.append(ypred)\n",
        "    \n",
        "    epoch_loss = np.mean(list_of_batch_losses)\n",
        "    val_losses.append(epoch_loss)\n",
        "    run[\"val/epoch_loss\"].log(epoch_loss)\n",
        "    \n",
        "    args = df['arg_id']\n",
        "    kps = df['key_point_id']\n",
        "    true_labels = df['label']\n",
        "    topics = df['topic']\n",
        "    stances = df['stance']\n",
        "    all_preds = []\n",
        "\n",
        "    for i in tqdm(range(len(y_preds))):\n",
        "      for p in y_preds[i]:\n",
        "        all_preds.append(p)\n",
        "            \n",
        "    print('Val evaluation....')\n",
        "    \n",
        "    pred_file = pd.DataFrame({\"arg_id\" : args, \"key_point_id\": kps, \"score\": all_preds})\n",
        "    args = {}\n",
        "    kps = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg] = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg][kp] = score\n",
        "\n",
        "    with open(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', 'w') as fp:\n",
        "        fp.write(json.dumps(args))\n",
        "        fp.close()\n",
        "    \n",
        "    arg_df, kp_df, labels_df = load_kpm_data(path_dataset, subset=\"dev\")\n",
        "    merged_df = get_predictions(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', labels_df, arg_df, kp_df)\n",
        "    \n",
        "    evaluate_predictions(merged_df,name = 'val')\n",
        "\n",
        "    return all_preds,true_labels, val_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X3ATAiOUSp_",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.969039Z",
          "iopub.execute_input": "2021-09-16T16:47:02.969419Z",
          "iopub.status.idle": "2021-09-16T16:47:02.995610Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.969385Z",
          "shell.execute_reply": "2021-09-16T16:47:02.994594Z"
        },
        "trusted": true
      },
      "source": [
        "def train_and_evaluate(train_dataset,df,model, filepath, model_name, batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = EPOCHS,accumulation_steps = ACCUMULATION_STEPS):\n",
        "  \n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "    \n",
        "  save_model = model_name+ '_SEED_'+ str(SEED) +'_dense_layer' +'_epoc_'+ str(epochs)+'_lr_'+ str(learning_rate)+'_b_s_'+ str(batch_size ) +'_accumulation_steps_'+ str(accumulation_steps) +'_input_type_kp_arg_topic' \n",
        "  save_predictions_name  = model_name+ '__TRAIN_PREDS_'+ 'SEED_'+ str(SEED) + '_dense_layer' +'_epoc_'+ str(epochs)+'_lr_'+ str(learning_rate)+'_b_s_'+ str(batch_size ) +'_accumulation_steps_'+ str(accumulation_steps) +'_input_type_kp_arg_topic'\n",
        "\n",
        "  training_dataloader = DataLoader(train_dataset, batch_size )\n",
        "  total_steps = len(training_dataloader) * epochs\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "  \n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "                                  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "    \n",
        "  model.zero_grad()\n",
        "\n",
        "  for epoch_i in tqdm(range(epochs)):\n",
        "    y_preds = []\n",
        "    y_val = []\n",
        "    list_of_batch_losses = []\n",
        "    epoch_iterator = tqdm(training_dataloader, desc=\"Iteration\")\n",
        "    model.train()\n",
        "    \n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      if model_name in model_with_no_token_types:\n",
        "        b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_features)\n",
        "      else:\n",
        "        b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_token_type,b_features)\n",
        "            \n",
        "      b_labels = torch.reshape(b_labels, (b_labels.shape[0], 1))\n",
        "      loss = criterion(outputs, b_labels.float())\n",
        "             \n",
        "      list_of_batch_losses.append(loss.detach().cpu().numpy())\n",
        "      run[\"train/batch_loss\"].log(np.mean(loss.detach().cpu().numpy()))\n",
        "      \n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "      ypred = outputs.detach().cpu().numpy()\n",
        "      b_labels = batch[-1].cpu().detach().numpy()\n",
        "      ypred = np.hstack(ypred)\n",
        "      y_preds.append(ypred)\n",
        "\n",
        "      if (step+1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "    epoch_loss = np.mean(list_of_batch_losses)\n",
        "    train_losses.append(epoch_loss)\n",
        "    run[\"train/epoch_loss\"].log(epoch_loss)\n",
        "    \n",
        "    args = df['arg_id']\n",
        "    kps = df['key_point_id']\n",
        "    true_labels = df['label']\n",
        "    topics = df['topic']\n",
        "    stances = df['stance']\n",
        "    all_preds = []\n",
        "    \n",
        "    for i in tqdm(range(len(y_preds))):\n",
        "      for p in y_preds[i]:\n",
        "        all_preds.append(p)\n",
        "\n",
        "    print('Train evaluation....')\n",
        "    \n",
        "    pred_file = pd.DataFrame({\"arg_id\" : args, \"key_point_id\": kps, \"score\": all_preds})\n",
        "    args = {}\n",
        "    kps = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg] = {}\n",
        "\n",
        "    for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "        args[arg][kp] = score\n",
        "\n",
        "    with open(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', 'w') as fp:\n",
        "        fp.write(json.dumps(args))\n",
        "        fp.close()\n",
        "    \n",
        "    arg_df, kp_df, labels_df = load_kpm_data(path_dataset, subset=\"train\")\n",
        "    merged_df = get_predictions(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', labels_df, arg_df, kp_df)\n",
        "    \n",
        "    evaluate_predictions(merged_df,name = 'train')\n",
        "    \n",
        "    _,_, val_epoch_loss = evaluate_model(val_dataset,df_val, model,  model_name, mode = 'val')\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    \n",
        "\n",
        "  torch.save(model, save_model_folder +save_model+'.pt')\n",
        "  run[\"model\"].upload(save_model_folder +save_model+'.pt')\n",
        "\n",
        "  print(\"Model is saved as : \",save_model)\n",
        "  print(\"Use this to load the model\")\n",
        "    \n",
        "  return save_model,train_losses, val_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "282z5CLGUSnw",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:02.996892Z",
          "iopub.execute_input": "2021-09-16T16:47:02.997320Z",
          "iopub.status.idle": "2021-09-16T16:47:03.011703Z",
          "shell.execute_reply.started": "2021-09-16T16:47:02.997232Z",
          "shell.execute_reply": "2021-09-16T16:47:03.010961Z"
        },
        "trusted": true
      },
      "source": [
        "def predict_model(test_dataset,df, save_model,model_name):\n",
        "  preds = []\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "  save_predictions_name = \"TEST_PREDS_\"+ '_SEED_'+ str(SEED) + save_model\n",
        "\n",
        "  model=torch.load(save_model_folder + save_model +'.pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\")\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      model.eval()\n",
        "\n",
        "      if model_name in model_with_no_token_types:\n",
        "        b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
        "        ypred = model(b_input_ids, b_input_mask,b_features)\n",
        "      else:\n",
        "        b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device),batch[4].to(device)\n",
        "        ypred = model(b_input_ids, b_input_mask,b_token_type,b_features)\n",
        "\n",
        "      ypred = ypred.cpu().numpy()\n",
        "      ypred = np.hstack(ypred)\n",
        "\n",
        "      preds.append(ypred)\n",
        "\n",
        "  args = df['arg_id']\n",
        "  kps = df['key_point_id']\n",
        "  all_preds = []\n",
        "\n",
        "  for i in tqdm(range(len(preds))):\n",
        "    for p in preds[i]:\n",
        "      all_preds.append(p)\n",
        "\n",
        "  pred_file = pd.DataFrame({\"arg_id\" : args, \"key_point_id\": kps, \"score\": all_preds})\n",
        "\n",
        "  args = {}\n",
        "  kps = {}\n",
        "\n",
        "  for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "    args[arg] = {}\n",
        "\n",
        "  for arg,kp,score in zip(pred_file['arg_id'],pred_file['key_point_id'],pred_file['score']):\n",
        "    args[arg][kp] = score\n",
        "\n",
        "  with open(path_predictions_folder + save_predictions_name + '_' + 'predictions.p.', 'w') as fp:\n",
        "    fp.write(json.dumps(args))\n",
        "    fp.close()\n",
        "\n",
        "  print(\"The predictions are stored in the file : \"+ path_predictions_folder  + save_predictions_name + '_' + 'predictions.p.')\n",
        "  \n",
        "  return path_predictions_folder + save_predictions_name + '_' + 'predictions.p.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:03.012966Z",
          "iopub.execute_input": "2021-09-16T16:47:03.013352Z",
          "iopub.status.idle": "2021-09-16T16:47:03.027735Z",
          "shell.execute_reply.started": "2021-09-16T16:47:03.013315Z",
          "shell.execute_reply": "2021-09-16T16:47:03.026912Z"
        },
        "trusted": true,
        "id": "bSnnN9XZRjX6"
      },
      "source": [
        "def train_additional_dataset(train_dataset, model, filepath, model_name, batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = EPOCHS,accumulation_steps = 1):\n",
        "  losses = []\n",
        "  save_model = model_name+ '_dense_layer' +'_epoc_'+ str(epochs)+'_lr_'+ str(learning_rate)+'_b_s_'+ str(batch_size ) + '_input_type_one'\n",
        "\n",
        "  training_dataloader = DataLoader(train_dataset, batch_size )\n",
        "  total_steps = len(training_dataloader) * epochs\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "  #no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
        "  \n",
        "  optimizer_grouped_parameters = [\n",
        "                                  {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "                                  {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "                                  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps = 1e-8)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps)\n",
        "\n",
        "  criterion = nn.BCELoss()\n",
        "    \n",
        "  model.zero_grad()\n",
        "  for epoch_i in tqdm(range(epochs)):\n",
        "\n",
        "    epoch_iterator = tqdm(training_dataloader, desc=\"Iteration\")\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "      \n",
        "      if model_name in model_with_no_token_types:\n",
        "        b_input_ids, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_features)\n",
        "      \n",
        "      else:\n",
        "        b_input_ids,b_token_type, b_input_mask,b_features, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device), batch[4].to(device)\n",
        "        outputs = model(b_input_ids, b_input_mask,b_features)\n",
        "          \n",
        "   \n",
        "      b_labels = torch.reshape(b_labels, (b_labels.shape[0], 1))\n",
        "\n",
        "      loss = criterion(outputs, b_labels.float())\n",
        "      loss = loss / accumulation_steps  \n",
        "      losses.append(loss)\n",
        "      \n",
        "      #optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      if (step+1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()\n",
        " \n",
        "  #torch.save(model, save_model_folder +save_model+'.pt')\n",
        "\n",
        "  #print(\"Model is saved as : \",save_model)\n",
        "  print(\"Model training on additional dataset is finished, but model not stored, since required ahead\")\n",
        "\n",
        "  return save_model,losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TysqDwtzUSgL",
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:03.029213Z",
          "iopub.execute_input": "2021-09-16T16:47:03.029622Z",
          "iopub.status.idle": "2021-09-16T16:47:03.037579Z",
          "shell.execute_reply.started": "2021-09-16T16:47:03.029578Z",
          "shell.execute_reply": "2021-09-16T16:47:03.036747Z"
        },
        "trusted": true
      },
      "source": [
        "def give_test_results(pred_file_path):\n",
        "  print('The strict and relaxed scores on the test set predictions are: ')\n",
        "  arg_df, kp_df, labels_df = load_kpm_data(gold_data_dir, subset=\"test\")\n",
        "  merged_df = get_predictions(pred_file_path, labels_df, arg_df, kp_df)\n",
        "  evaluate_predictions(merged_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-09-16T16:47:51.940974Z",
          "iopub.execute_input": "2021-09-16T16:47:51.941406Z"
        },
        "trusted": true,
        "id": "2jqKywAYRjX6",
        "outputId": "de40386f-c409-474f-b19b-766d2d7e596e",
        "colab": {
          "referenced_widgets": [
            "d084e3270e3847d0871a3a9d5e59ad13",
            "15bd8975bf684968a3d6d6437fc118d9"
          ]
        }
      },
      "source": [
        "save_model, train_additional_dataset_losses = train_additional_dataset(sts_train_dataset, model, save_model_folder, model_name = 'bartlarge', batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = 6, accumulation_steps = ACCUMULATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d084e3270e3847d0871a3a9d5e59ad13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Iteration:   0%|          | 0/502 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15bd8975bf684968a3d6d6437fc118d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfN_37KIUSdn",
        "trusted": true
      },
      "source": [
        "save_model, train_losses, val_losses = train_and_evaluate(train_dataset,df_train, model, save_model_folder, model_name = 'bartlarge', batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, epochs = EPOCHS, accumulation_steps = ACCUMULATION_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCgWwXDmeYFc",
        "trusted": true
      },
      "source": [
        "test_preds_path = predict_model (test_dataset,df_test, save_model,model_name = 'bartlarge')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDgGNi8leZvD",
        "trusted": true
      },
      "source": [
        "give_test_results(test_preds_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVVLl89xX1B4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}